---
title: "DCDM Project Log"
author: "Alicia"
format: html
editor: visual
---

## DCDM Project Log

```{r}
rm(list = ls())

# Install and load necessary packages
# install.packages(c("dplyr", "purrr", "readr")) # only do this once
library(dplyr)
library(purrr)
library(readr)

# Define the working directory path 
path = "/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
```

## Combining the Raw Data in a Table

The dataset comprises thousands of CSV files. The key fields include: analysis_id, pvalue, parameter_name, gene_symbol, mouse_strain, parameter_id, gene_accession_id, mouse_life_stage.

To ensure efficient quality checks and facilitate further analysis, it is essential to combine all these files into a single, unified table based on their common fields.

Hereâ€™s a function that reads data from a file and transforms it into a row for a larger table:

```{r}
# Function to read a file and transform its contents into a single row
read_and_transform <- function(file) {
  
  # Read the file
  data <- read.csv(file, header = FALSE, col.names = c("field", "value"))
  # - header = FALSE: Tells R to treat the first row as regular data, not column names.
  # - col.names = c("field", "value"): Assigns consistent column names to facilitate clear and reliable subsetting in subsequent processing.

  # Convert all field names to lowercase for uniformity
  data$field <- tolower(data$field)
  
  # Transpose the 'value' column so that fields become column names
  transformed_data <- t(data$value) %>% 
    as.data.frame() %>% 
    setNames(data$field)
  
  # Rearrange columns to place 'analysis_id' as the first column
  transformed_data <- transformed_data %>%
    relocate(analysis_id, .before = everything())

  # Return the transformed data as a single-row data frame
  return(transformed_data)
}
```

The function can now be applied to all CSV files in the `raw_data` directory to combine them into a single table using the `map_dfr()` function from the **`purrr`** package.

```{r}
# List all CSV files in the raw_data directory
all_files <- list.files(paste0(path, "data/raw_data"), pattern = "\\.csv$", full.names = TRUE)

# Apply the read_and_transform function to each file and combine results into a single data frame
data_table <- purrr::map_dfr(all_files, read_and_transform)
# Display the first 6 rows of the data table
View(data_table)

# Save the combined table to a new CSV file
write.csv(data_table, paste0(path, "/temporary/data_table.csv"), row.names = FALSE)
```

## Quality Control Using the SOP

The experiment data comes with an SOP specifying constraints for each field, including data type, minimum and maximum values for numerical data, length for strings, and specific values for certain fields. Data quality can be assessed by comparing the raw data to these predefined constraints.

First, have a look at the SOP to identify the constraints.

```{r}
SOP <- read.csv(paste0(path, "data/metadata/IMPC_sop.csv"))
SOP
```

For each column, we need to check similar things, like whether the data is the correct type (e.g., string or number), whether it has the right length (for strings) or size (for floats), and whether it matches allowed values, like for `mouse_strain` and `mouse_life_stage`. We may also need to find missing values. For each issue, we want to print the value and its index. To make this process less repetitive, we can create reusable functions.

*Here, it would be good to check if the functions work as intended on values that should output an error (e.g. fake table containing rows each violating a single constraint for a parameter).*

```{r}
# These functions check if values meet specific criteria and return the indices of values that do not meet the criteria.

# Function to validate values based on the SOP type ("String" or "Float")
# For strings, it checks the string length; for numeric values, it checks the value directly.
validate_type_minmax <- function(x, dataType, min_val, max_val) {
  if (tolower(dataType) == "string") {
    which(sapply(x, nchar) < min_val | sapply(x, nchar) > max_val)
  } else if (tolower(dataType) == "float") {
    which(as.numeric(x) < min_val | as.numeric(x) > max_val)
  } 
}

# Check if values in a list are within the allowed values of a given vector
check_allowed_values <- function(values, allowed_values) {
  which(!values %in% allowed_values) # Find indices of invalid values
}

# Function to combine multiple lists of indices and return unique indices
combine_unique_indices <- function(...) {
  sort(unique(unlist(list(...))))
}

# Function to print the indices and values were issues were identified
print_invalid_indices_and_values <- function(x, variable_name, invalid_indices) {
  # Count the number of issues
  num_issues <- length(invalid_indices)
  
  # Print the contextual message
  cat("The '", variable_name, "' column has ", num_issues, " issue(s).\n", sep = "")
  
  # Print indices and values if there are any issues
  if (num_issues > 0) {
    cat("Indices:\n")
    print(invalid_indices)
    cat("Values:\n")
    print(x[invalid_indices])
  }
}
```

Now that we have defined these functions, we can loop through the data table fields, extract the corresponding constraints from the SOP, and validate them using the functions.

```{r}
# Extract field names from the data table
field_names <- colnames(data_table)
print(field_names)
  
# Loop through each field name
for (field_name in field_names) {
    cat("Processing field:", field_name, "\n")
    
    # Extract the corresponding row in the SOP
    sop_row <- SOP[SOP$dataField == field_name, ]
    
    # Access the column by name
    column <- data_table[[field_name]]
    
    # Validate the column based on SOP constraints
    SOP_constraints <- validate_type_minmax(column, sop_row$dataType, sop_row$minValue, sop_row$maxValue)
    
  # Check field-specific constraints
  # Initialize a variable for checking valid values
  field_specific_check <- integer(0) # Default to an empty vector

  if (field_name == "mouse_strain") {
    valid_mouse_strains <- c("C57BL", "B6J", "C3H", "129SV")
    field_specific_check <- check_allowed_values(column, valid_mouse_strains)
  } else if (field_name == "mouse_life_stage") {
    valid_mouse_life_stages <- c(
      "E12.5", "E15.5", "E18.5", "E9.5", 
      "Early adult", "Late adult", "Middle aged adult"
    )
    field_specific_check <- check_allowed_values(column, valid_mouse_life_stages)
  } else if (field_name == "analysis_id") {
    # Check if unique
    field_specific_check <- which(duplicated(column))
  } else if (field_name == "gene_symbol") {
    # Check if in title format
    field_specific_check <- which(!grepl("^[A-Z][a-zA-Z0-9]*$", column))
  }
    
    # Combine all indices of invalid values
    indices <- combine_unique_indices(SOP_constraints, field_specific_check)
    
    # Print the invalid indices and their corresponding values
    print_invalid_indices_and_values(column, field_name, indices)
}
```

*The next step is to handle the issues. Do we cap the p-values at 1? Do we remove the data from incorrect strains?*

## Determining an Appropriate Database Schema

Load the extra files

```{r}
rootDir <- "/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
```

```{bash}
rootDir="/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/metadata/"
# Step 1: Replace semicolons (;) with commas (,)
# Some files may use semicolons as delimiters. This replaces them with commas for consistent formatting.
sed 's/;/,/g' "${rootDir}IMPC_procedure.txt" | 

# Step 2: Remove extra double quotes
# - `s/^"//`: Removes double quotes at the beginning of a line.
# - `s/"$//`: Removes double quotes at the end of a line.
# - `s/" "/,/g`: Replaces any double quote followed by a space and another double quote (" ") with a comma.
sed 's/^"//;s/"$//;s/" "/,/g' |

# Step 3: Enclose the 3rd column (description) in double quotes
# - `^([0-9]+,[^,]+,)`: Matches the first two columns (line number and procedure ID).
# - `(.*)`: Matches the 3rd column (description), which may include commas or spaces.
# - `(, (true|TRUE|True|false|FALSE|False), [0-9]{5})$`: Matches the last two columns (boolean and 5-digit number).
# - Replacement `\1"\2"\3`: Reconstructs the line with the 3rd column enclosed in double quotes.
sed -E 's/^([0-9]+,[^,]+,)(.*)(, (true|TRUE|True|false|FALSE|False), [0-9]{5})$/\1"\2"\3/' |

# Step 4: Remove the "line_number" column
# - `s/^line_number,//`: Removes the header "line_number" from the first line and its corresponding data in all other lines.
sed -E 's/^line_number,//' > "${rootDir}cleaned_IMPC_procedure.txt"
```

```{r}
# Load the cleaned procedure data
procedure <- read.table(
  file = paste0(rootDir, "data/metadata/cleaned_IMPC_procedure.txt"), # File path
  header = TRUE,                 # First row contains column names
  sep = ",",                     # Fields are separated by commas
  quote = '"',                   # Handle quoted fields
  stringsAsFactors = FALSE,      # Prevent character columns from converting to factors
  check.names = FALSE            # Keep column names as-is, even if they contain spaces or special characters
)

# Display the loaded data in a tabular format
View(procedure)
```

```{bash}
rootDir="/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/metadata/"

sed -E 's/^"//; s/"$//; s/" "/,/g;' "${rootDir}IMPC_parameter_description.txt" |
sed -E 's/^([0-9]{1,4},[0-9]{5}, )(.*)(,[^,]+, IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})$/\1"\2"\3/' |
sed -E 's/^x//' > "${rootDir}cleaned_IMPC_parameter_description.txt"

```

```{r}
# Load the cleaned parameter data
parameter_description <- read.table(
  file = paste0(rootDir, "data/metadata/cleaned_IMPC_parameter_description.txt"), # File path
  header = TRUE,                 # First row contains column names
  sep = ",",                     # Fields are separated by commas
  stringsAsFactors = FALSE,      # Prevent character columns from converting to factors
  check.names = FALSE            # Keep column names as-is, even if they contain spaces or special characters
)

# Display the loaded data in a tabular format
View(parameter_description)
```

```{bash}

```

```{r}

# Load the cleaned parameter data
disease_information <- read.table(
  file = paste0(rootDir, "data/metadata/disease_information.txt"), # File path
  header = TRUE,                 # First row contains column names
  sep = ",",                     # Fields are separated by commas
  quote = '"',                   # Handle quoted fields
  stringsAsFactors = FALSE,      # Prevent character columns from converting to factors
  check.names = FALSE            # Keep column names as-is, even if they contain spaces or special characters
)

# Display the loaded data in a tabular format
View(disease_information)

```
