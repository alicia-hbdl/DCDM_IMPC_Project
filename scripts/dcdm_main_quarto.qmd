---
title: "DCDM Project Log"
author: "Alicia"
editor: visual
format: html
---

# TRE Egress (Ella)

## **Data Security Awareness Training**

1.  Account created through the [NHS e-Learning for Healthcare portal](https://portal.e-lfh.org.uk/Component/Details/544034)
2.  Course *Data Security Awareness - Level 1* :
    -   Completed the training and passed the examination

## **CREATE TRE Egress**

1.  Logged on to the Virtual Machine
    -   This [link](https://portal.er.kcl.ac.uk/vdiresource/281) was used to access a virtual machine (specific machine: er-tre-352-vm07) through the KCL e-research portal er_tre_msc_dcdm Virtual Desktops
    -   Logged into Virtual Machine with my KCL login
2.  From the Desktop, clicked on the "Data_Egress_Request” icon, which lead to the KCL TRE Data Egress Portal
3.  Clicked on the "Egress request tab", which led to a form to fill out with details about the data requested
    -   Selected "Group3.zip" within the file picker, requesting our specific dataset
    -   Answered questions about the classification, point of origin, end-point destination, purpose for egress, and type of data, specific to our dataset
4.  Submitted the Egress Request
5.  The egress was approved, and the data was made available to us as a zip file (Group3.zip) on the CREATE HPC in the /scratch_tmp/grp/msc_appbio/7BBG1003/CW folder

## Moving the Data

Though we had access to the zip file of our data, we had no permissions on it so were unable to move or copy it to our own data. This required me to download the file to my own system using scp, then re-upload the file to a our group project folder using scp. I checked the file sizes before and after using the `ls -l` command, to ensure no data was corrupted in the process.

```{bash}
# downloading the data locally from the HPC
# scp -i ~/.ssh/msckey  k24064085@hpc.create.kcl.ac.uk:/scratch_tmp/grp/msc_appbio/7BBG1003/CW/Group3.zip ~/Desktop/applied_bioinformatics/DCDM/group_project/raw_data/Group3

# uploading the data into our DCDM_group3 directory on the HPC
# scp -i ~/.ssh/msckey  ~/Desktop/applied_bioinformatics/DCDM/group_project/raw_data/Group3.zip k24064085@hpc.create.kcl.ac.uk:/sratch_tmp/grp/msc_appbio/DCDM_group3
```

# Data Cleaning

```{r}
rm(list = ls())

# Install and load Tidyverse packages
#install.packages(c("tidyverse", "janitor"))
library(tidyverse)
library(janitor)
```

```{r}
# Define the working directory path in r
# Alicia
#rootDir = "/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
# Sanj 
rootDir = "/Users/sanjanasrinivasan/Desktop/DCDM_IMPC_Project/"
# Ella
# rootDir = "/Users/ellajadealex/Desktop/applied_bioinformatics/DCDM/group_project/DCDM_IMPC_Project/"
# Sama
#rootDir = "/Users/sama/Desktop/DCDM_Proj2/"
# Esta
#rootDir = "~/Desktop/working_directory/DCDM_project"
```

## Cleaning the Raw Data Files

### Quality Control/Cleaning Using the SOP (Esta)

The experiment data includes an SOP that defines constraints for each field, such as data type, minimum and maximum values for numerical fields, string lengths, and specific allowable values. Data quality is assessed by comparing the raw data against these constraints.

In this step, we have cleaned all CSV files by modifying the row names to match those specified in the SOP.

#### Setting-Up the Environment

We begin by defining the directory paths and the log file path.

```{r}
# Defing directory paths
raw_data_dir <- paste0(rootDir, "/data/raw_data") # Input raw data directory
updated_data_dir <- paste0(rootDir, "/data/updated_data") # Cleaned analysis data directory (field names only)
log_file_mod <- paste0(rootDir, "/scripts/row_name_modification.log") # Log file for modifications
log_file_valid <- paste0(rootDir, "/scripts/row_name_validation.log")   # Log file for results
SOP_path <- paste0(rootDir, "/data/metadata/IMPC_SOP.csv") # SOP path

# Creates output directory if it doesn't already exist & supresses warnings if it does exist
dir.create(updated_data_dir, showWarnings = FALSE)

# Clear the log files if they exist or create them otherwise
file.create(log_file_mod)
file.create(log_file_valid)
```

Then we load the SOP file. *Here, I have normalized the row names to lowercase although this isn't strictly necessary for this specific file.*

```{r}
# Load the SOP file and normalize row names
expected_row_names <- trimws(read_csv(SOP_path, show_col_types = FALSE)$dataField)

# Check normalized row names
expected_row_names
```

#### Modify the field names

With the correct files loaded and directories specified, we define a function to perform the following tasks:

1.  Identify and correct spelling mistakes in the row names. In this case, only uppercase-to-lowercase transformations are needed;

2.  Add any missing rows, assigning `NA` to the second column for those rows. This step is performed after correcting any misspelled row names;

3.  Reorder the rows to align with the SOP layout.

```{r}
# Function to validate, update, and reorder row names
process_file <- function(file_path) {
  # Read the CSV file as headerless
  data <- tryCatch(
    read_csv(file_path, col_names = FALSE, show_col_types = FALSE), 
    error = function(e) return(NULL))
  
  # If file reading fails, skip processing
  if (is.null(data)) return(FALSE)
  
  # Normalize row names
  data[[1]] <- tolower(data[[1]])
  
  # Identify missing rows and add them, input NA value for second column
  missing_rows <- setdiff(expected_row_names, data[[1]])
  if (length(missing_rows) > 0) {
    data <- bind_rows(data, tibble(V1 = missing_rows, V2 = NA))
  }
  
  # Reorder rows based on expected_row_names
  data <- arrange(data, match(data[[1]], expected_row_names))
  
  # Save the updated file as headerless
  write_csv(data, file.path(updated_data_dir, basename(file_path)), col_names = FALSE)
 
  return(TRUE)
}
```

We now apply this function to all data files. This process will generate an `updated_data` folder containing the cleaned files and a log file in the main directory.

```{r}
# Process files and log results
validation_results <- sapply(list.files(raw_data_dir, pattern = "\\.csv$", full.names = TRUE), process_file)

# Log summary
summary_message <- sprintf("Validation Summary:\nTotal files checked: %d\nTotal updated files: %d\n",
                           length(validation_results), sum(validation_results))
write(summary_message, log_file_mod)
message(summary_message)
```

The summary message displays the number of files validated out of the total number in the `raw_data` folder. This information is shown on the console and saved to the log file. The log file, now streamlined to include only this summary, initially contained detailed row name differences between the files and the SOP, based on a subset of 600 files.

#### Verify the field names

#### This code duplicates the logic used to modify the files, making it somewhat redundant and unable to detect field name issues. I have kept it as a separate optional script to check for any potential collating errors - this script is a remnant of the data exploration code I used to identify the steps needed during data cleaning. {style="color: red"}

After modifying the files, we run a final validation script to confirm that there are no unexpected row name differences remaining. This ensures the data aligns perfectly with the SOP.

```{r}
# Function to validate a file against the SOP layout and column count
validate_file <- function(file_path) {
  # Read the file
  data <- tryCatch(
    read_csv(file_path, col_names = FALSE, show_col_types = FALSE),
    error = function(e) return(NULL))
  
  # Skip files that could not be read
  if (is.null(data)) return(FALSE)
  
  # Check column count
  if (ncol(data) != 2) {
    return(list(valid = FALSE, message = "Incorrect column count"))
  }
  
  # Normalize row names and validate against SOP
  row_names <- tolower(data[[1]])
  missing_rows <- setdiff(expected_row_names, row_names)
  extra_rows <- setdiff(row_names, expected_row_names)
  
  # Create validation message
  if (length(missing_rows) > 0 || length(extra_rows) > 0) {
    message <- c(
      if (length(missing_rows) > 0) paste("Missing row names:", paste(missing_rows, collapse = ", ")),
      if (length(extra_rows) > 0) paste("Extra row names:", paste(extra_rows, collapse = ", "))
    )
    return(list(valid = FALSE, message = paste(message, collapse = "\n")))
  }
  
  list(valid = TRUE, message = "File is valid")
}

# Process all files
validation_results <- lapply(list.files(updated_data_dir, pattern = "\\.csv$", full.names = TRUE), function(file_path) {
  result <- validate_file(file_path)
  log_message <- paste(basename(file_path), ":", result$message, "\n")
  cat(log_message, file = log_file, append = TRUE)
  result$valid
})

# Summarize and log results
valid_files <- sum(unlist(validation_results))
summary_message <- sprintf(
  "Validation Summary:\nTotal files checked: %d\nTotal valid files: %d\n", 
  length(validation_results), valid_files
)
cat(summary_message, file = log_file_valid, append = TRUE)
message(summary_message)

```

This code can be run both locally and on an HPC with minimal adjustments, such as loading individual `dplyr` and `readr` packages instead of the entire `tidyverse`.

To check the differences between the raw and cleaned files (quick visual check to see if we are doing something).

```{bash}
#| eval: false

diff -yr --suppress-common-line /path/to/raw_directory /path/to/cleaned_directory
```

```{r}
# Remove unused values to unclutter the environment
rm(log_file_mod, log_file_valid, raw_data_dir, updated_data_dir, valid_files, total_files, validation_results, files, summary_message, expected_row_names)
```

### Combining the Raw Data in a Table

The dataset consists of thousands of CSV files with key fields: `analysis_id`, `gene_accession_id`, `gene_symbol`, `mouse_strain`, `mouse_life_stage`, `parameter_id`, `parameter_name`, and `pvalue`.

To streamline data cleaning and enable further analysis, these files must be combined into a single table based on their shared fields.

Below is a function that reads a file and converts its data into a row for the unified table:

```{r}
#| eval: false

# Define a function to process a single file
process_file <- function(file_path) {
  read.csv(file_path, header = FALSE, col.names = c("field", "value")) %>%
    # Transpose the data so that fields become column names and values form the row
    t() %>% 
    # Convert to dataframe (needed by the map_dfr function)
    as.data.frame() %>% 
    # Set the first row (after transposing) as the column names
    setNames(.[1, ]) %>%
    # Remove the first row since it is now used as column names
    slice(-1) 
}

# List all CSV files in the updated_data directory
all_files <- list.files(path = updated_data_dir, pattern = "\\.csv$", full.names = TRUE) 

# Apply the function to each file and combine the results into a single data frame
analysis_table <- map_dfr(all_files, process_file)

# Specify the output path
output_path <- file.path(rootDir, "data/cleaned/analysis_table.txt")

# Save the combined table to a text file
write.table(analysis_table, file = output_path, sep = ",", quote = TRUE, row.names = FALSE)

# Display the first 6 rows of the combined table
head(analysis_table)
```

### 2nd Column Data Cleaning... (To be done after collating data on HPC??? - Esta)

For each column, we need to check similar things, like whether the data is the correct type (e.g., string or number), whether it has the right length (for strings) or size (for floats), and whether it matches allowed values, like for `mouse_strain` and `mouse_life_stage`. We may also need to find missing values. For each issue, we want to print the value and its index. To make this process less repetitive, we can create reusable functions.

Potential workflow:

-   Start by selecting a column or field, such as `analysis_id`, and verifying that its values are in the correct format (e.g., strings).

-   Check whether the length of each value falls within the required range (e.g., `min < nchar < max`).

-   For missing or incorrect values, decide whether to update them to `NA` or remove problematic rows, particularly if fields like `mouse_strain` or `life_stage` contain invalid entries.

-   For numerical fields such as `p-value`, it may be useful to cap values, setting anything above 1 to 1 and below 0 to 0.

-   Additionally, identify and manually correct any typos to ensure the data aligns with expectations.

## Cleaning the Metadata Files

### `IMPC_procedure.txt`

Each row in the `IMPC_procedure.txt` file contains a line number, procedure name, description, a mandatory boolean field (TRUE/FALSE), and the impcParameterOrigId. The raw file has several issues, including unnecessary double quotes, misinterpreted commas, and duplicated field names.

A first formatting cleanup step addresses these issues to allow the file to be loaded as a table in R.

```{r}
temporary_path <- paste0(rootDir, "data/cleaned/reformatted_IMPC_procedure.txt")

# Reformat for proper table loading
procedure <- read_lines(paste0(rootDir, "data/metadata/IMPC_procedure.txt")) %>% 
  # Replace double quotes between fields with commas
  str_replace_all('" "', ",") %>%
  # Remove leading and trailing double quotes from each line
  str_remove_all('^"|"$') %>%
  # Wrap descriptions containing commas in quotes 
  str_replace('^([0-9]+,[^,]+,)(.*)(, (TRUE|FALSE), [0-9]{5})$', '\\1"\\2"\\3') %>%
  # Remove the "line_number" field name 
  str_remove('line_number,') %>%
  write_lines(temporary_path) # Save the reformatted lines to a new file
```

A second step cleans the table to ensure consistency and reduce redundancy. This includes correcting HTML encoding, trimming spaces, renaming columns for consistency with other tables, and converting empty fields to `NA`.

The table contains over 5,000 rows but only 50 distinct procedures, as each row represents a single parameter tested within a procedure. To reduce redundancy, procedures with identical names, descriptions, and mandatory values were assigned a shared `procedure_id`, while their `impcParameterOrigId` values were stored as a list. This preserves mappings needed for integration with the parameter table.

Some procedures, such as "MicroCT E14.5-E15.5" and "MicroCT E14.5-E15.5 Analysis," were merged as they referred to the same test. For others ("Experimental Design," "Housing and Husbandry," "Electrocardiogram (ECG)," and "Viability Primary Screen,") missing descriptions were filled using existing values, and conflicts were resolved by selecting the most appropriate description. These adjustments were finalized before grouping to ensure accurate consolidation of procedures.

```{r}
# Clean and standardize the data
procedure <- read_csv(temporary_path, col_names = TRUE) %>%
  # Apply these transformations to all values in the table
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%                   # Trim leading and trailing whitespace
      str_replace_all(c(               # Replace HTML entities
        "&amp;" = "&",
        "&nbsp;" = " ")) %>% 
      na_if("")                        # Replace empty strings with NA
  )) %>% 
  # Rename columns for consistency
  rename(
    procedure_id = procedureId, 
    procedure_name = name,
    procedure_description = description,
    is_mandatory = isMandatory,
    parameter_mapping = impcParameterOrigId
  ) %>% 
  # Manually standardize specific procedure names and descriptions
  mutate(
    procedure_name = case_when(
      procedure_name == "MicroCT E14.5-E15.5 Analysis" ~ "MicroCT E14.5-E15.5",
      TRUE ~ procedure_name
    ),
    procedure_description = case_when(
      procedure_name == "Experimental design" ~ "experimental_design",
      procedure_name == "Housing and Husbandry" ~ "housing_and_husbandry",
      procedure_name == "Viability Primary Screen" ~ "Assess the viability of mutant mice of each sex and zygosity.",
      procedure_name == "Electrocardiogram (ECG)" ~ "To provide a high throughput method to obtain Electrocardiograms in a conscious mouse.",
      is.na(procedure_description) ~ tolower(str_replace_all(procedure_name, " ", "_")), # Replace NA descriptions with procedure_name (underscored)
      TRUE ~ procedure_description
    )
  ) %>% 
  # Group by identical procedure_name, is_mandatory, and procedure_description
  group_by(procedure_name, is_mandatory, procedure_description) %>%
  summarise(
    # Assign a unique ID to all rows within the group
    procedure_id = cur_group_id(),
    # Store all parameter_mapping values as a list
    parameter_mapping = list(parameter_mapping),
    .groups = "drop" # Remove grouping 
  )  %>% 
  unnest(parameter_mapping) # Expand the list column into multiple rows

# Preview the resulting data
head(procedure)

# Save the cleaned data to a new CSV file
write_csv(procedure, paste0(rootDir, "data/cleaned/cleaned_IMPC_procedure.csv"))
```

The resulting table retains a row for each parameter tested within a procedure; however, a single procedure consistently shares the same name, description, boolean value, and is assigned a common identifier. This ensures that a simplified, non-redundant procedures table can be created in the database while also allowing for the creation of a clear join table that leverages the original identifiers for accurate mapping.

```{r}
file.remove(temporary_path)
rm(temporary_path)
```

### `IMPC_parameter_description.txt (Sama)`

###### I noticed that the current code includes some debugging elements, which might make it a bit harder to follow and maintain. To streamline it, I went ahead and created a consolidated version using tidyverse principles. This version removes the debugging code to make it tidier and more reproducible. I’ve kept all your original code intact, so nothing is lost, and my consolidated version is included below. When you have some time, it would be great if you could review it and confirm that it aligns with the intended logic and functionality. {style="color: red"}

Using readLines(), the metadata file is read and formatting lines as per specified pattern.

```{r}
#| eval: false

parameter_path <- paste0(rootDir, "/data/metadata/IMPC_parameter_description.txt")
parameter_outputFile = paste0(rootDir, "/temporary/IMPC_Parameter_Description.txt")

# Read the file
lines <- readLines(parameter_path)

lines_cleaned <- lines %>%
  # Replace spaces between quotes with commas
  str_replace_all('" "', '","') %>%
  # Reformat the lines as per the specified pattern
  str_replace_all('^(\\d{1,4},\\d{5}, )([A-Z][^,]+(?:\\([^)]*\\))*(?:, )?)([a-z][^,]*\\([^)]*\\)?,?)(,[^,]+, IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})$', 
                '\\1"\\2"\\3"\\4')

# Write the modified lines to a txt file
writeLines(lines_cleaned, parameter_outputFile)

```

Initital observation:

The metadata contains five columns; line_number, impcParameterOrigId, name, description, parameterId.

-   The entire metadata is being read as a single column where all the rows are stored as strings.
-   They all are under a label 'x'. First two columns are merged as one and is not separated by comma.

Opening the saved .txt file of the metadata with a specific format using read_delim() function and specifying that it is delimited by commas.

```{r}
#| eval: false

# Read the metadata file (assuming tab-delimited, change delimiter if needed)
pd <- read_delim(parameter_outputFile, delim = ",")

pd
```

Observations:

-   There may be a redundancy between name and description, as they often describe the same parameter but in different styles.
-   They are all presented as characters, although some columns contains numeric values, this is because the colnames hasn't been specified yet.

The function below checks for commas in paratheses or any extra commas that will cause an abnormal splits amongst the columns.

```{r}
#| eval: false

# Function to identify rows with extra commas or commas inside parentheses
identify_comma_issues <- function(pd, expected_columns) {
  pd <- pd %>%
    rowwise() %>%
    mutate(
      # Count the number of commas in the row
      comma_count = str_count(paste(across(everything()), collapse = ","), ","),
      
      # Expected number of commas (expected_columns - 1)
      expected_commas = expected_columns - 1,
      
      # Flag rows with extra commas
      has_extra_commas = comma_count > expected_commas,
      
      # Check for commas inside parentheses
      has_commas_in_parentheses = str_detect(paste(across(everything()), collapse = " "), "\\([^\\)]*?,[^\\)]*?\\)")
    ) %>%
    ungroup()  # Ungroup after rowwise operations
  
  # Filter rows with issues
  issues <- pd %>%
    filter(has_extra_commas | has_commas_in_parentheses) %>%
    select(everything(), comma_count, expected_commas, has_extra_commas, has_commas_in_parentheses)
  
  return(issues)
}

# Define the number of expected columns in your dataset
expected_columns <- 5

# Call the function on your dataset
comma_issues <- identify_comma_issues(pd, expected_columns)

# Output the results
if (nrow(comma_issues) > 0) {
  print("Rows with potential comma issues:")
  print(comma_issues)
} else {
  print("No rows with comma issues found.")
}

view(comma_issues)
```

41 issues has been found. There are commas in parentheses that is causing unnecessary splits. To fix it, the commas in parentheses can be replaced by ;

```{r}
#| eval: false

# Replace commas inside parentheses in all relevant columns (before separating)
pd <- pd %>%
  mutate(across(everything(), ~ str_replace_all(., '\\(([^)]*)\\)', function(m) {
    # Replace commas with semicolons inside the parentheses
    str_replace_all(m, ',', ';')
  })))


```

```{r}
#| eval: false

# Separate 'x', the single column into multiple columns.
pd <- pd %>%
  separate(x, into = c("line_number", "impcParameterOrigId", "name", "description", "parameterId"), sep = ",", extra = "merge", fill = "right")

# Remove the first row as it's been used for column names.
pd <- pd %>% slice(-1)

# Renaming columns in the metadata dataframe to match the SOP. 
pd <- pd %>%
  rename(
    line_number = line_number,
    IMPC_parameter_orig_id = impcParameterOrigId,
    parameter_name = name,
    description = description,
    parameter_id = parameterId
  )

# Remove the 'line_number' column
pd <- pd %>%
  select(-line_number)

```

Using separate(), it will split the single 'x' column into different columns based on commas.

-   extra = "merge": If there are more fields than columns, merge the extra fields into the last column.
-   fill = "right": Adds NA to empty columns when there are fewer fields than columns.

Each columns are then renamed to correspond to SOP.

```{r}
#| eval: false

# Convert the first two columns line_number and IMPC_parameter_orig_id to integers
pd <- pd %>%
  mutate(
    IMPC_parameter_orig_id = as.numeric(IMPC_parameter_orig_id)
  )

str(pd)
```

This script validates columns in a metadata table against constraints defined in an SOP. It then retrieves the column names from the metadata table and prints them for reference and loops through each field name in order to check against the SOP. Furthermore, it skips columns with no SOP definition.

```{r}
#| eval: false

# Function to validate values based on the SOP type ("String" or "Float")
validate_type_minmax <- function(x, dataType, min_val, max_val) {
  if (tolower(dataType) == "string") {
    which(sapply(x, nchar) < min_val | sapply(x, nchar) > max_val)
  } else if (tolower(dataType) == "float") {
    which(as.numeric(x) < min_val | as.numeric(x) > max_val)
  } 
}

# Check if values in a list are within the allowed values of a given vector
check_allowed_values <- function(values, allowed_values) {
  which(!values %in% allowed_values) # Find indices of invalid values
}

# Function to combine multiple lists of indices and return unique indices
combine_unique_indices <- function(...) {
  sort(unique(unlist(list(...))))
}

# Function to print the indices and values where issues were identified
print_invalid_indices_and_values <- function(x, variable_name, invalid_indices) {
  # Count the number of issues
  num_issues <- length(invalid_indices)
  
  # Print the contextual message
  cat("The '", variable_name, "' column has ", num_issues, " issue(s).\n", sep = "")
  
  # Print indices and values if there are any issues
  if (num_issues > 0) {
    cat("Indices:\n")
    print(invalid_indices)
    cat("Values:\n")
    print(x[invalid_indices])
  }
}

# Extract field names from the data table (metadata)
field_names <- colnames(pd)
print(field_names)

# Loop through each field name to apply SOP checks
for (field_name in field_names) {
  cat("Processing field:", field_name, "\n")
  
  # Extract the corresponding row in the SOP for each field
  sop_row <- SOP[SOP$dataField == field_name, ]
  
  if (nrow(sop_row) == 0) {
    next  # Skip if no SOP rule exists for the field
  }

# Access the column by name from the metadata
  column <- pd[[field_name]]
  
  # Validate the column based on SOP constraints (Type, Min, Max)
  SOP_constraints <- validate_type_minmax(column, sop_row$dataType, sop_row$minValue, sop_row$maxValue)
  
  # Initialize a variable for field-specific checks
  field_specific_check <- integer(0)  # Default to an empty vector
  
  # Combine all indices of invalid values from SOP and field-specific checks
  invalid_indices <- combine_unique_indices(SOP_constraints, field_specific_check)
  
  # Print the invalid indices and their corresponding values (if any issues)
  print_invalid_indices_and_values(column, field_name, invalid_indices)
}


```

-   Five issues have been found in parameter_name column. Some of them are in question format, which may need to be standerdaised..

-   There are 31 issues with the parameter_id column. They contain parts of description column, alongside of the parameter_id. For example, "in English, IMPC_HOU_076_001" should only be "IMPC_HOU_076_001". This is caused by extra commas present within "description" column.

-   There are some missing values in some parts of the dataset.

Extracting any texts that belongs to the description column from the parameter_id, only keeping the unique id starting with "IMPC\_", and moving them to the description column.

```{r}
#| eval: false

# Function to move text before IMPC_ into description and keep IMPC_ code in parameter_id
fix_parameter_id <- function(pd) {
  pd <- pd %>%
    mutate(
      # Extract text before the IMPC code and keep it in description
      description = paste(description, str_extract(parameter_id, "^(.*?)(?=IMPC_)"), sep = " "),  

      # Keep only the IMPC code in parameter_id (get everything starting from IMPC_)
      parameter_id = str_extract(parameter_id, "IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3}")
    )
  
  return(pd)
}

# Apply the function to the metadata
pd <- fix_parameter_id(pd)
```

```{r}
#| eval: false

# Trim whitespaces and then replace blank strings with NA
pd <- pd %>%
  mutate(
    parameter_name = na_if(str_trim(parameter_name), ""),
    description = na_if(str_trim(description), ""),
  )

```

```{r}
#| eval: false

# Define the output path for the cleaned metadata
output_file_parameter <- paste0(rootDir, "data/cleaned/cleaned_IMPC_parameter_description.txt")

# Save the cleaned metadata
write.csv(pd, file = output_file_parameter, row.names = FALSE)

# Completion message
cat("Cleaned metadata has been saved to:", output_file_parameter, "\n")

```

Load the cleaned parameter data.

```{r}
#| eval: false

# Load the cleaned parameter data
parameter_description <- read.table(
  file = paste0(rootDir, "data/cleaned/cleaned_IMPC_parameter_description.txt"), # File path
  header = TRUE,                 # First row contains column names
  sep = ",",                     # Fields are separated by commas
  stringsAsFactors = FALSE,      # Prevent character columns from converting to factors
  check.names = FALSE            # Keep column names as-is, even if they contain spaces or special characters
)

# Display the loaded data in a tabular format
head(parameter_description)
```

Questions: does the parameter_name content needs to be standardised?

The problematic areas the script found:\
\[1\] " Difference in Conditioning Post-shock and Conditioning Baseline % Freezing"\
\[2\] " Nutrition - do you know the composition of the diet (average based on mass)"\
\[3\] " Nutrition - do you know the composition of the diet (average based on calorific content)"\
\[4\] " Nutrition - average composition (based on calorific content) - Carbohydrate"\
\[5\] " Microbiological status - is your unit positive for any of the pathogens tested"

### Consolidated Script for `IMPC_parameter_description.txt` {style="color: red"}

The same issue as with IMPC_procedure.txt is observed with the parameter `IMPC_parameter_description.txt` file. Therefore, the same two steps are performed: 1) correct the formatting to be able to load it as a table in R 2) clean the values inside the table.

```{r}
# Read the raw IMPC_parameter_description.txt file line-by-line
parameter <- read_lines(paste0(rootDir, "data/metadata/IMPC_parameter_description.txt")) %>%
  .[-1] %>%                                   # Remove the first line, which is assumed to be a header "x"
  discard(~ . == "") %>%                      # Remove any completely empty lines
  str_remove_all('^"|"$') %>%                 # Remove leading and trailing double quotes from each line
  # Remove inconsistent newline characters to ensure each procedure data point forms a single row  
  str_c(collapse = "\n") %>%                  # Combine all lines into a single string for processing
  str_replace_all(c('" "'= ",","\n" = " " )) %>%  # Replace double quotes with commas and remove newline characters
  str_replace_all(c(
    '(IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})' = '\\1\n',  # Add a newline character after each matched IMPC identifier
    'line_number,impcParameterOrigId, name, description, parameterId' = 
      'parameter_mapping, parameter_name, parameter_description, parameter_id\n'  # Rename the header row for consistency
  )) %>%
  str_split("\n") %>%                         # Split the single string back into lines based on the newline character
  unlist() %>%                                # Convert the resulting list into a flat character vector
  str_trim() %>%                              # Trim whitespace from the beginning and end of each line
  # Replace commas within parentheses with semicolons
  str_replace_all('\\(([^)]*)\\)', function(x) str_replace_all(x, ',', ';')) %>% 
  # Match the desired table structure (line number, procedure_id, name, description, parameter_id) and add quotes 
  str_replace_all(
    '^([1-9][0-9]{0,3}),([0-9]{4,5}), ?([^,]+), ?(.*), ?(IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})$', 
    '"\\2","\\3","\\4","\\5"') %>%
  write_lines(paste0(rootDir, "data/cleaned/formatted_IMPC_parameter_description.txt")) # Save the cleaned lines to a new file

# Preview the first few lines of the cleaned file
head(parameter)
```

```{r}
# This section is not finished yet
# Load and clean the parameter data
parameter <- read_csv(
  paste0(rootDir, "data/cleaned/formatted_IMPC_parameter_description.txt")
) %>%
  # Clean the data: trim whitespace and replace empty strings with NA
  mutate(
    across(
      everything(),
      ~ .x %>%
        str_trim() %>%   # Remove leading and trailing whitespace
        na_if("")        # Convert empty strings to NA
    )
  ) %>%
  # Parameters are tested in a single procedure (this was checked using procedure file) so we can keep one record per unique parameter
  distinct(
    parameter_name, parameter_description, parameter_id,
    .keep_all = TRUE
  ) # Reduces rows from 5326 to 3625

# Identify rows where parameter_id is duplicated
duplicated_parameters <- parameter %>%
  filter(parameter_id %in% parameter_id[duplicated(parameter_id)])
# Duplicated parameters are due to parameter names or descriptions not being the same, while the ID is - we can look up the ID to check which version to keep, and manually remove the other version (still needs to be done; )
# Display the rows with duplicated parameter_id
duplicated_parameters


# Validate parameter_name and parameter_id
parameter_SOP <- parameter %>%
  mutate(
    valid_parameter_name = nchar(parameter_name) >= 2 & nchar(parameter_name) <= 74,  # Check name length
    valid_parameter_id = nchar(parameter_id) >= 15 & nchar(parameter_id) <= 18,       # Check ID length
    unique_parameter_id = !duplicated(parameter_id)                                   # Check uniqueness of parameter_id
    )

# Filter rows with invalid parameter_name, parameter_id, or non-unique parameter_id
invalid_entries <- parameter_SOP %>%
  filter(!valid_parameter_name | !valid_parameter_id | !unique_parameter_id) %>%
  select(parameter_name, parameter_id, valid_parameter_name, valid_parameter_id, unique_parameter_id)

# Output the invalid entries or a success message
if (nrow(invalid_entries) > 0) {
  cat("Invalid entries detected:\n")
  print(invalid_entries)
} else {
  cat("All entries are valid and parameter IDs are unique.\n")
}
```

### `disease_information.txt (Sanj)`

The Disease_information.txt file contains metadata with four key columns: `disease_id`, `disease_term`, `gene_accession_id`, and `phenodigm_score`.

**Key Issues:**

-   Unecessary rows and formatting (redundant "x" row at the top, row numbers and surrounding double quotes embedded within data) –\> Remove unnecessary rows, row numbers and extraneous quotes

-   The `disease_term` column contains commas misinterpreted as separators -\> Combine split fields to ensure disease_term remains intact

-   `phenodigm_score` contains non-numeric characters and extra spaces -\> Clean non-numeric values and safely convert to numeric format

-   Row numbers and quotes appear in the `disease_id` column -\> Clean `disease_id` to remove row numbers and quotes

-   

Step 1: Clean the raw metadata file, organising it into four columns

```{r}

# 1a) Read the raw lines
raw_lines <- readLines((paste0(rootDir, "data/metadata/Disease_information.txt")))

# 1b) Remove the first 'x' row (unnecessary header or placeholder)
cleaned_lines <- raw_lines[-1]


# 1c) Remove row numbers and surrounding quotes from each line
#     - '^\\d+\\"\\s' looks for leading digits, a quote, and a space
#     - '^"|"$' removes any leading or trailing quotes
cleaned_lines <- cleaned_lines %>%
  str_remove_all('^\\d+\\"\\s') %>%
  str_remove_all('^"|"$')


# 1d) Function to properly combine split fields 
process_split_row <- function(row_line) {
  # Split by commas that are *not* enclosed in quotes:
  split_row <- str_split(
    row_line, 
    pattern = ",\\s*(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)")[[1]]
  
  # If the row is incorrectly split into > 4 parts, we paste everything except the first & last two as the disease_term
  if (length(split_row) > 4) {
    disease_term <- paste(split_row[2:(length(split_row) - 2)], collapse = ", ")
    out <- c(
      split_row[1], 
      disease_term, 
      split_row[length(split_row) - 1], 
      split_row[length(split_row)]
    )
    return(out)
  } else {
    # If it’s already at 4 parts, just return as is
    return(split_row)
  }
}

# Apply our row processing to each line
split_data <- map(cleaned_lines, process_split_row)

# Check if any rows still have a number of columns that is not equal to 4
problematic_rows <- which(map_int(split_data, length) != 4)
if (length(problematic_rows) > 0) {
  cat("Problematic rows found at indices:", problematic_rows, "\n")
} else {
  cat("All rows split correctly into four columns.\n")
}

```

Step 2: Load data into a Table & Clean Values within Rows/Columns

```{r}
# 2a) Convert the list-of-vectors into a tibble
disease_data <- split_data %>%
  map(~ set_names(as.list(.), c("disease_id", "disease_term", "gene_accession_id", "phenodigm_score"))) %>%
  bind_rows()


# 2b) Trim and remove extra bits from phenodigm_score; convert to numeric
disease_data <- disease_data %>%
  mutate(
    # Remove leading/trailing spaces
    phenodigm_score = str_trim(phenodigm_score),
    # Remove non-digit and non-dot chars
    phenodigm_score = str_replace_all(phenodigm_score, "[^0-9\\.]", ""),
    # Convert safely to numeric
    phenodigm_score = as.numeric(phenodigm_score)
  )


# 2c) Clean up disease_id if extra quotes or row numbers still remain
disease_data <- disease_data %>%
  mutate(
    disease_id = str_remove(disease_id, '^\\d+\\"\\s'),
    disease_id = str_remove_all(disease_id, '^"|"$')
  )


# 2d) The very first row is still a leftover header, remove it
disease_data <- disease_data[-1, ]  # Drop the first row


# Verifying changes
head(disease_data) 

```

Step 3: Saving and Loading the Final Data

```{r}

# Specify the path to the cleaned directory
output_path <- paste0(rootDir, "data/cleaned/disease_information.txt")

# Save data_frame as a tab-delimited text file
write.table(disease_data, file = output_path, sep = "\t", row.names = FALSE, quote = FALSE)

# Load the final cleaned data
disease_information <- read.table(
  file = output_path, 
  header = TRUE,          # Use the first row as column names 
  sep = "\t",             # Tab-delimited
  stringsAsFactors = FALSE) # Avoid character columns from converting to factors

# Verify the loaded data
str(disease_information)

#Display the cleaned and loaded data in tabular format
head(disease_information)

```

# Collating (Ella)

# MySQL Database

```{sql}
#| eval: false

-- Check if MySQL is running
ps -ef | grep mysql

-- Connect to MySQL
mysql -u root -p --local-infile=1
-- local-infile=1 enables data import from local files using LOAD DATA LOCAL INFILE.
```

## Create Database and Schemas

```{sql}
#| eval: false

-- Create the database
CREATE DATABASE IMPCDb;

-- Select the database
USE IMPCDb;
```

```{sql}
#| eval: false

-- Create tables without foreign keys first

-- Table: Gene
CREATE TABLE Genes (
    gene_id VARCHAR(11) PRIMARY KEY,           -- SOP: 9 to 11 alphanumeric characters
    gene_symbol VARCHAR(13)                    -- SOP: 1 to 13 alphanumeric characters
) COMMENT = 'Stores gene identifiers and symbols. Referenced by Analyses and PhenodigmScores (one-to-many).';


-- Table: ProcedureTable (Procedure is a reserved keyword in SQL)
CREATE TABLE Procedures (
    procedure_id INT PRIMARY KEY,              -- Unique identifier for each procedure (impcParameterOrigId)
    procedure_name VARCHAR(47),                -- Max length: 47 characters
    procedure_description VARCHAR(1090),       -- Max length: 1090 characters
    procedure_isMandatory BOOLEAN NOT NULL     -- No missing values; TRUE/FALSE
        DEFAULT FALSE
) COMMENT = 'Stores procedure details (name, description, mandatory). Linked to Parameters via a many-to-many join.';


-- Table: Disease
CREATE TABLE Diseases (
    disease_id VARCHAR(50) PRIMARY KEY,        -- Unique identifier for each disease
    disease_term VARCHAR(150)                  -- Descriptive term for the disease
) COMMENT = 'Stores disease identifiers and terms. Linked to Genes in PhenodigmScores (many-to-many).';


-- Table: ParameterGroupings
CREATE TABLE ParameterGroupings (
    grouping_id INT PRIMARY KEY AUTO_INCREMENT,   -- Automatically increments for each new row
    grouping_name VARCHAR(50)                     -- Name of the parameter group
) COMMENT = 'Defines groups of parameters. Linked to Parameters via a many-to-many join.';


-- Table: Parameter
CREATE TABLE Parameters (
    parameter_id VARCHAR(18) PRIMARY KEY,          -- SOP: 15 to 18 characters
    parameter_name VARCHAR(74),                    -- SOP: 2 to 74 characters
    parameter_description VARCHAR(1000)            -- Descriptive field for the parameter
) COMMENT = 'Stores parameter metadata. Linked to Procedures and ParameterGroupings (many-to-many) and referenced by Analyses (one-to-many).';

-- Create tables with foreign keys

-- Table: Analysis
CREATE TABLE Analyses (
    analysis_id VARCHAR(15) PRIMARY KEY,           -- SOP: 15 alphanumeric character string
    gene_id VARCHAR(11),                           -- SOP: 9 to 11 alphanumeric characters
    mouse_life_stage VARCHAR(17),                  -- SOP: 4 to 17 characters
    mouse_strain VARCHAR(5),                       -- SOP: 3 to 5 alphanumeric characters
    parameter_id VARCHAR(18),                      -- Foreign key to Parameter table
    p_value FLOAT,                                 -- SOP: Float from 0 to 1
    FOREIGN KEY (gene_id) REFERENCES Genes(gene_id),
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id)
) COMMENT = 'Stores analysis results linking Genes and Parameters (many-to-one), including p-values and strain info.';


-- Table: GeneDisease
CREATE TABLE PhenodigmScores (
    phenodigm_id INT AUTO_INCREMENT PRIMARY KEY,

    disease_id VARCHAR(50),                        -- Foreign key to Disease table
    gene_id VARCHAR(11),                           -- Foreign key to Gene table
    phenodigm_score FLOAT,                         -- Phenodigm association score
    FOREIGN KEY (disease_id) REFERENCES Diseases(disease_id),
    FOREIGN KEY (gene_id) REFERENCES Genes(gene_id)
) COMMENT = 'Stores gene-disease association scores (many-to-many) linking Genes and Diseases.';


-- Create join tables

-- Table: ParameterProcedure
CREATE TABLE parameterXprocedure (
    parameter_id VARCHAR(18),                      -- Foreign key to Parameter table
    procedure_id INT,                              -- Foreign key to ProcedureTable
    PRIMARY KEY (parameter_id, procedure_id),      -- Composite primary key to ensure uniqueness
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id),
    FOREIGN KEY (procedure_id) REFERENCES Procedures(procedure_id)
) COMMENT = 'Join table linking Parameters to Procedures (many-to-many).';


-- Table: ParameterGroup
CREATE TABLE parameterXgroup (
    grouping_id INT,                               -- Foreign key to ParameterGroupings table
    parameter_id VARCHAR(18),                      -- Foreign key to Parameter table
    PRIMARY KEY (grouping_id, parameter_id),       -- Composite primary key to ensure uniqueness
    FOREIGN KEY (grouping_id) REFERENCES ParameterGroupings(grouping_id),
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id)
) COMMENT = 'Join table linking Parameters to ParameterGroupings (many-to-many).';
```

## Populate Database

To populate the database, the schema should first be examined to identify which attributes correspond to each table. Data should then be organized into separate CSV or TSV files, with each file representing a single table. Each file is expected to include a header row (optional but recommended) that matches the column names defined in the database schema.

Once a CSV file has been created for each table, the data can be loaded directly into MySQL using the `LOAD DATA INFILE` command.

-   **`FIELDS TERMINATED BY ','`**: Indicates that fields are separated by commas.

-   **`OPTIONALLY ENCLOSED BY '"'`**: Handles strings enclosed in double quotes.

-   **`IGNORE 1 LINES`**: Skips the header row in the CSV, if present.

### Procedure Table

```{r}
procedures <- procedure %>%
  select(procedure_id, procedure_name, procedure_description, is_mandatory) %>%  # Select specific columns
  distinct() # Keep only unique rows (this shrinks the table from 5311 rows to 50 rows)

# Save the table as CSV 
write_csv(procedures, paste0(rootDir, "/data/collated/Procedures.csv"))
rm(procedures)
```

```{sql}
#| eval: false
-- Path to tables 
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/cleaned/Procedures.csv'
INTO TABLE Procedures
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

### Genes Table

```{r}
# Load analysis table 
analysis <- read_csv(cleaned_procedure_path)
disease <- read_delim(paste0(rootDir, "data/cleaned/cleaned_Disease_information.txt"), delim = "\t")
head(disease)

# Subset the table, ensure unique rows, convert gene symbols to title case, and rename
genes <- analysis %>%
  clean_names() %>% 
  mutate(gene_symbol = str_to_title(gene_symbol)) %>%  # Convert gene_symbol to title case 
  select(gene_accession_id, gene_symbol) %>%          # Select specific columns
  distinct() %>%                                      # Keep only unique rows
  rename(gene_id = gene_accession_id)                 # Rename columns

# Find `gene_id`s in `disease` table that are not in `genes`
missing_genes <- disease %>%
  select(gene_accession_id) %>%                # Extract gene IDs from `disease`
  rename(gene_id = gene_accession_id) %>%
  distinct() %>%                               # Ensure unique rows
  filter(!gene_id %in% genes$gene_id)          # Find those not in `genes`

# Add missing `gene_id`s to `genes` with `gene_symbol` as NULL; here we could manually search them and add them! 
genes <- genes %>%
  bind_rows(missing_genes %>% 
  mutate(gene_symbol = NA)) %>% # Add missing genes with NA for gene_symbol
  distinct()                    # Ensure no duplicates

head(genes)

# Save the table as CSV 
write.csv(genes, file = paste0(rootDir, "/data/collated/Genes.csv"), row.names = FALSE, quote = TRUE)
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Genes.csv'
INTO TABLE Genes
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

### Parameters Table

```{r}
# Load analysis table 
parameter <- read_delim(paste0(rootDir, "data/cleaned/cleaned_IMPC_parameter_description.txt"), delim = ",")


# Select the subset of columns required for the Parameters table
parameters <- parameter %>%
  clean_names() %>% 
  mutate_all(~ str_trim(.)) %>%                       # Trim leading/trailing whitespace
  mutate_all(~ ifelse(. %in% c("", "NA"), NA, .)) %>% # Replace empty strings or "NA" with actual NA
  select(parameter_id, name, description) %>%          # Select only the specific columns needed
  rename(parameter_id = parameter_id,                  # Rename the columns
         parameter_name = name,                      
         parameter_description = description) %>%
  group_by(parameter_id) %>%                          # Group by parameter_id
  slice_min(nchar(parameter_name), with_ties = FALSE) %>% # Keep the row with the shortest parameter_name
  ungroup()                                     # Remove grouping

parameters[(nchar(parameters$parameter_name))> 74, ]
# Save the table as CSV 
write.csv(parameters, file = paste0(rootDir, "/data/collated/Parameters.csv"), row.names = FALSE, quote = TRUE)
```

*\*Some of the IDs appeared to be duplicated, but upon inspection, the rows contained equivalent values (e.g., for IMPC_HIS_010_001: "Eye with optic nerve - Severity score" and "Eye - Severity score"). Given the same identifier and the similarity of these terms, it is reasonable to assume that they refer to the same parameter, and only one of the duplicates was uploaded to the database.*

![](images/Screenshot%202024-12-18%20at%2015.27.00.png)

*Some values are truncated, which aligns with the SOP specifying a maximum of 74 characters. However, the maximum number of characters in the dataset is 88, exceeding the allowed limit.*

![](images/Screenshot%202024-12-18%20at%2015.26.12.png)

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Parameters.csv'
INTO TABLE Parameters
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

### Disease Table

```{r}
# Select the subset of columns required for the Parameters table
diseases <- disease %>%
  clean_names() %>% 
  select(disease_id, disease_term) %>%          # Select only the specific columns needed
  distinct()

write_csv(diseases, paste0(rootDir, "/data/collated/Diseases.csv")) 
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Diseases.csv'
INTO TABLE Diseases
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

### PhenoDigm Score Table

```{r}
# Select the subset of columns required for the Parameters table
phenodigm_scores <- disease %>%
  clean_names() %>% 
  select(disease_id, gene_accession_id, phenodigm_score) %>%          # Select only the specific columns needed
  distinct()

write_csv(phenodigm_scores, paste0(rootDir, "/data/collated/PhenodigmScores.csv")) 
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/PhenodigmScores.csv'
INTO TABLE PhenodigmScores
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
LINES TERMINATED BY '\n' 
IGNORE 1 LINES
(disease_id, gene_id, phenodigm_score); -- so that the phenodigm_id is created dynamically
```

## parameterXprocedure

```{r}
# Join the parameters table and the procedure table using "parameter_mapping" and "procedure_id"
parameterXprocedure <- procedure %>%
  left_join(parameter, by = c("parameter_mapping" = "procedure_id")) %>%
  select(procedure_id, parameter_id) %>%  # Retain only the relevant columns
  distinct() # Ensure no duplicates (this shrinks the table from 5311 rows to 3600 rows)

# Save the resulting table as a CSV file
write_csv(parameterXprocedure, paste0(rootDir, "/data/collated/parameterXprocedure.csv"))
# Remove unused files 
rm(parameterXprocedure)
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/parameterXprocedure.csv'
INTO TABLE parameterXprocedure
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 LINES; -- so that the phenodigm_id is created dynamically
```

### Analysis Table

```{r}
# Load analysis table 
analysis <- read_delim(paste0(rootDir, "data/cleaned/analysis_table.txt"), delim = ",")

head(analysis)
analyses <- analysis %>%
  clean_names() %>% 
  mutate_all(~ str_trim(.)) %>%                       # Trim leading/trailing whitespace
  mutate_all(~ ifelse(. %in% c("", "NA"), NA, .)) %>% # Replace empty strings or "NA" with actual NA
  select(analysis_id, gene_accession_id, mouse_strain, mouse_life_stage, parameter_id, pvalue)   %>%  
    rename(# Select only the specific columns needed  rename(parameter_id = parameter_id,                  # Rename the columns
      p_value = pvalue)   %>%  
    distinct()
head(analyses)
write_csv(analyses, paste0(rootDir, "/data/collated/Analyses.csv"))
rm(analyses)
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Analyses.csv'
INTO TABLE Analyses
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 LINES; -- so that the phenodigm_id is created dynamically
```

## Determining Groupings

To reduce the parameter space the  collaborator would like to start grouping parameters together based on phenotype test similarity and/or naming similarity. Design the database to reflect new groupings for weight,  images and brain parameters, plus at least 3 others of your choice (refer to below). 

In addition to the data provided, each group must  find at least 3 other parameter groupings from the parameter list and add this to the  database. We encourage you to find as many additional parameter groups as you can and  time permits, however only 3 more is compulsory. parameters

```{r}
parameters <- parameters %>%
  mutate(
    parameter_group = case_when(
      str_detect(parameter_name, regex(
        "weight|density|fat|lean|BMI|BMC|mass|composition|bone|length|adipose|lipid|body size|size|growth|skeletal|area|volume", 
        ignore_case = TRUE)) ~ "Weight",
      str_detect(parameter_name, regex(
        "image|screenshot|microscope|pdf|lacz|waveform|ogram|resolution|visual|pixel|scan|photo|imaging|bit depth|microscopy|contrast", 
        ignore_case = TRUE)) ~ "Images",
      str_detect(parameter_name, regex(
        "brain|cortex|hippo|thalamus|amygdala|lobe|cere|spinal|pituitary|striatum|hypothal|neural|neuronal|pons|medulla|ganglion|nerve|enceph|ventricle|choroid", 
        ignore_case = TRUE)) ~ "Brain",
      str_detect(parameter_name, regex(
        "heart|cardiac|vascular|blood|vein|aort|arter|HR|stroke|ejection|QT|circulation|pulse|pressure|rate|systolic|diastolic|respiration|ECG|cardio|valve", 
        ignore_case = TRUE)) ~ "Cardiovascular"
      )
)

parameters %>%
  group_by(parameter_group) %>%
  summarise(total_parameters = n()) %>%
  arrange(desc(total_parameters))

parameters$parameter_name[is.na(parameters$parameter_group)]

```

## Create dump

```{bash}
/opt/homebrew/bin/mysqldump IMPCDb -u root -p > /Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/database/IMPCDb.dump
```

# R/Shiny

**Test R/Shiny 1:** They want the ability to select a particular knockout mouse and visualise the statistical scores of all phenotypes tested.

```{sql}
#| eval: false
SELECT Analyses.p_value, Parameters.parameter_name 
FROM Analyses 
JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id 
WHERE Analyses.gene_id IN (
    SELECT gene_id 
    FROM Genes 
    WHERE gene_symbol IN ('Ifi204', 'Mettl21c', 'Ier5', 'Abca4')
)
ORDER BY Analyses.p_value ASC;
```

![](images/Screenshot%202024-12-21%20at%2016.21.52.png)

**Test R/Shiny 2:** The collaborator also wishes to visualise the statistical scores of all knockout mice for a selected phenotype.

```{sql}
#| eval: false
SELECT Analyses.p_value, Genes.gene_symbol 
FROM Analyses 
JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id 
JOIN Genes ON Analyses.gene_id = Genes.gene_id 
WHERE Parameters.parameter_name IN (
    'Facial Cleft',
    'Lens Opacity',
    'Triglycerides',
    'Skin color - tail',
    'Potassium',
    'HRV',
    'Hindbrain morphology',
    'Aspartate aminotransferase',
    'Tibia length',
    'Left total retinal thickness',
    'Embryo Size',
    'Effector CD4+ T helper cells - % of live leukocytes (Panel A)',
    'Limb morphology',
    'Coat - color - back',
    'Aggression',
    'Joints'
)
ORDER BY Analyses.p_value ASC;
```

![](images/Screenshot%202024-12-21%20at%2016.22.09.png)

**Test R/Shiny 3:** The collaborator would like to visualise clusters of genes with similar phenotype scores.  

```{sql}
#| eval: false

SELECT Genes.gene_symbol, Parameters.parameter_name, Analyses.p_value FROM Analyses JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id JOIN Genes ON Analyses.gene_id = Genes.gene_id; 
```

![](images/Screenshot%202024-12-21%20at%2016.22.33.png)
