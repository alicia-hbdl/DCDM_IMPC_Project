---
title: "DCDM Project Log"
author: "Alicia"
editor: visual
format: html
---

# TRE Egress (Ella)

## **Data Security Awareness Training**

1.  Account created through the [NHS e-Learning for Healthcare portal](https://portal.e-lfh.org.uk/Component/Details/544034)
2.  Course *Data Security Awareness - Level 1* :
    -   Completed the training and passed the examination

## **CREATE TRE Egress**

1.  Logged on to the Virtual Machine
    -   This [link](https://portal.er.kcl.ac.uk/vdiresource/281) was used to access a virtual machine (specific machine: er-tre-352-vm07) through the KCL e-research portal er_tre_msc_dcdm Virtual Desktops
    -   Logged into Virtual Machine with my KCL login
2.  From the Desktop, clicked on the "Data_Egress_Request” icon, which lead to the KCL TRE Data Egress Portal
3.  Clicked on the "Egress request tab", which led to a form to fill out with details about the data requested
    -   Selected "Group3.zip" within the file picker, requesting our specific dataset
    -   Answered questions about the classification, point of origin, end-point destination, purpose for egress, and type of data, specific to our dataset
4.  Submitted the Egress Request
5.  The egress was approved, and the data was made available to us as a zip file (Group3.zip) on the CREATE HPC in the /scratch_tmp/grp/msc_appbio/7BBG1003/CW folder

## Moving the Data

Though we had access to the zip file of our data, we had no permissions on it so were unable to move or copy it to our own data. This required me to download the file to my own system using scp, then re-upload the file to a our group project folder using scp. I checked the file sizes before and after using the `ls -l` command, to ensure no data was corrupted in the process.

```{bash}
# downloading the data locally from the HPC
# scp -i ~/.ssh/msckey  k24064085@hpc.create.kcl.ac.uk:/scratch_tmp/grp/msc_appbio/7BBG1003/CW/Group3.zip ~/Desktop/applied_bioinformatics/DCDM/group_project/raw_data/Group3

# uploading the data into our DCDM_group3 directory on the HPC
# scp -i ~/.ssh/msckey  ~/Desktop/applied_bioinformatics/DCDM/group_project/raw_data/Group3.zip k24064085@hpc.create.kcl.ac.uk:/sratch_tmp/grp/msc_appbio/DCDM_group3
```

# Data Cleaning

```{r}
rm(list = ls())

# Install and load Tidyverse packages
#install.packages(c("tidyverse", "janitor", "fs"))
library(tidyverse)
library(janitor)
library(fs)

# ADD DIRECTORY STRUCTURE HERE 
```

```{r}
# Define the working directory path in r
# Alicia
rootDir = "/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
# Sanj 
#rootDir = "/Users/sanjanasrinivasan/Desktop/DCDM_IMPC_Project/"
# Ella
# rootDir = "/Users/ellajadealex/Desktop/applied_bioinformatics/DCDM/group_project/DCDM_IMPC_Project/"
# Sama
#rootDir = "/Users/sama/Desktop/DCDM_Proj2/"
# Esta
# rootDir = "~/Desktop/working_directory/DCDM_project"
```

## Cleaning the Raw Data Files

### Quality Control/Cleaning Using the SOP (Esta)

The experiment data includes an SOP that defines constraints for each field, such as data type, minimum and maximum values for numerical fields, string lengths, and specific allowable values. Data quality is assessed by comparing the raw data against these constraints.

In this step, we have cleaned all CSV files by modifying the row names to match those specified in the SOP.

#### Setting-Up the Environment

We begin by defining the directory paths and the log file path.

```{r}
# Defing directory paths
raw_data_dir <- paste0(rootDir, "data/raw_data/") # Input raw data directory
updated_data_dir <- paste0(rootDir, "data/updated_data/") # Input raw data directory

log_file <- paste0(rootDir, "scripts/analysis_cleaning_overall.log") # Log file for validation & modifications

SOP_path <- paste0(rootDir, "data/metadata/IMPC_SOP.csv") # SOP path

# Clear the log files if they exist or create them otherwise
file.create(log_file)


```

Then we load the SOP file. *Here, I have normalized the row names to lowercase although this isn't strictly necessary for this specific file.*

```{r}
# Load the SOP file and normalize and check row names
expected_row_names <- trimws(read_csv(SOP_path, show_col_types = FALSE)$dataField)

# Check normalized row names
expected_row_names
```

#### Validating the number of columns

To quickly identify any any delimation errors, we can validate the number of columns per CSV file (each file should have 2 columns).

```{r}
# Function to validate column count
validate_columns <- function(file_path) {
  # Read the file and check its column count
  analysis <- tryCatch(
    read_csv(file_path, col_names = FALSE, show_col_types = FALSE),
    error = function(e) return(NULL)
  )
  
  if (is.null(analysis)) return(FALSE)
  
  # Return TRUE if the file has exactly 2 columns, otherwise FALSE
  return(ncol(analysis) == 2)
}
```

#### Modify the field names

With the correct files loaded and directories specified, we define a function to perform the following tasks:

1.  Identify and correct spelling mistakes in the row names. In this case, only uppercase-to-lowercase transformations are needed;

2.  Add any missing rows, assigning `NA` to the second column for those rows. This step is performed after correcting any misspelled row names;

3.  Reorder the rows to align with the SOP layout.

Here, I have also included the column count validation function defined above. This means the results will all be presented in the same file.

```{r}
# Function to validate, update, and reorder row names
combine_files <- function(file_path) {
  # Validate column count
  if(!validate_columns(file_path)){
    message(paste("File", basename(file_path), "does not have exactly 2 columns. Skipping."))
    return(FALSE)
  }
  # Read the CSV file as headerless
  analysis <- tryCatch(
    read_csv(file_path, col_names = FALSE, show_col_types = FALSE), 
    error = function(e) return(NULL))
  
  # If file reading fails, skip processing
  if (is.null(analysis)) return(FALSE)
  
  # Normalize row names
  analysis[[1]] <- tolower(analysis[[1]])
  
  # Identify missing rows and add them, input NA value for second column
  missing_rows <- setdiff(expected_row_names, analysis[[1]])
  if (length(missing_rows) > 0) {
    missing_data <- tibble(!!colnames(analysis)[1] := missing_rows)
    for (i in 2:ncol(analysis)) {
      missing_data[[colnames(analysis)[i]]] <- NA
    }
    analysis <- bind_rows(analysis, missing_data)
  }
  
  # Reorder rows based on expected_row_names
  analysis <- arrange(analysis, match(analysis[[1]], expected_row_names))
  
  # Save the updated file as headerless
  write_csv(analysis, paste0(updated_data_dir, basename(file_path)), col_names = FALSE)
 
  return(TRUE)
}
```

We now apply this function to all data files. This process will generate an `updated_data` folder containing the cleaned files and a log file in the main directory.

```{r}
# Process files and log results
validation_results <- dir_ls(path = raw_data_dir, glob = "*.csv") %>%
  map_dfr(combine_files) #%>%
 # map(as.integer) %>%
 # unlist()

# Log summary
summary_message <- sprintf("Validation Summary:\nTotal files checked: %d\nTotal updated files: %d\n", length(validation_results), sum(validation_results))
write_lines(summary_message, log_file)
message(summary_message)
```

The log file, now streamlined to include only this summary, initially contained detailed row name differences between the files and the SOP, based on a subset of 600 files.

This code can be run both locally and on an HPC with minimal adjustments, such as loading individual `dplyr` and `readr` packages instead of the entire `tidyverse`.

To check the differences between the raw and cleaned files (quick visual check to see if we are doing something).

```{bash}
#| eval: false
diff -yr --suppress-common-line /path/to/raw_directory /path/to/cleaned_directory
```

```{r}
# Remove unused values to unclutter the environment
rm(raw_data_dir, validation_results, summary_message, expected_row_names)
```

### Combining the Raw Data in a Table

The data-set consists of thousands of CSV files with key fields: `analysis_id`, `gene_accession_id`, `gene_symbol`, `mouse_strain`, `mouse_life_stage`, `parameter_id`, `parameter_name`, and `pvalue`.

To streamline data cleaning and enable further analysis, these files must be combined into a single table based on their shared fields.

Below is a function that reads a file and converts its data into a row for the unified table:

```{r}
# Define a function to process a single file
clean_files <- function(file_path) {
  read_csv(file_path, show_col_types = FALSE) %>%
    # Transpose the data so that fields become column names and values form the row
    t() %>% 
    # Convert to dataframe (needed by the map_dfr function)
    as.data.frame() %>% 
    # Set the first row (after transposing) as the column names
    setNames(.[1, ]) %>%
    # Remove the first row since it is now used as column names
    slice(-1) %>%
    mutate(analysis_id = rownames(.), .before = 1)
}

# Apply the function to each file and combine the results into a single data frame
analysis <- dir_ls(path = updated_data_dir, glob = "*.csv") %>%
  map_dfr(clean_files) %>%
  write_csv(paste0(rootDir, "data/raw_data/analysis_table.txt"))

# Display the first 6 rows of the combined table
head(analysis)
```

### Cleaning the Collated Raw Data (Esta)

For each column, we need to check similar things, like whether column values are the right lengths (for strings) or size (for floats), and whether the results matche allowed values, like for `mouse_strain` and `mouse_life_stage`.We also need to check if certain variables are perfectly correlated with one another.

First we load the data and generate a log file.

```{r}
# Read the collated file
analysis <- read_csv(paste0(rootDir, "data/raw_data/analysis_table.txt"), show_col_types = FALSE)

# Custom logging function
log_message <- function(message) {
  cat(message, "\n", file = log_file, append = TRUE)
  message(message) # Optionally display in the console
}

# Start logging
log_message("=== Data Validation and Transformation Log ===")
```

Check if variable data types match those specified in the SOP - all character (or string) data types except for pvalue (numeric or float).

```{r}
# Extract expected data types
expected_data_types <- read_csv(SOP_path, show_col_types = FALSE)$dataType %>% 
  recode("String" = "character", "Float" = "numeric") # Convert datatypes to those recognised by R

# Get actual data types
data_types <- sapply(analysis, class)

# Identify and report mismatches
type_mismatches <- tibble(
  variable = names(data_types),
  actual_type = unname(data_types),
  expected_type = expected_data_types[names(data_types)]
) %>%
  filter(actual_type != expected_type)

# Log results
if (nrow(type_mismatches) > 0) {
  log_message("Data type mismatches found:")
  log_message(capture.output(print(type_mismatches)))
} else {
  log_message("All data types match the SOP specification.")
}

rm(data_types, expected_data_types)
```

Check if `analysis_id` values are all 15 characters long and all unique.

```{r}
### Validate 'analysis_id'
invalid_ids <- analysis %>%
  filter(nchar(analysis_id) != 15 | duplicated(analysis_id))

log_message(if (nrow(invalid_ids) == 0) {
  "All analysis_id values are valid."
} else {
  paste("Invalid analysis_id values found:", nrow(invalid_ids))
})
```

Check if `gene_accession_id` values are all between 9-11 characters long - According to the SOP they should all be unique but our script shows over 190,000 duplicated results. We have chosen to consider duplicated gene_accession_ids valid.

###### basically when you create a knockout mouse, you knock out a specific gene, but then that gene goes through a workflow of procedures (like electrocardiogram) and then within an ecg, multiple parameters are measured (qrs, heart rate, ejection fraction, etc e.g), so for each gene, you automatically have many files because each file is a single parameter. so it’s totally normal to have duplicate IDs in teh analysis file, however, the unique in this case in the sop means taht the gene ids are specific ways to refer to a unique gene, so they are already used in some way as foreign key in teh analysis table {style="color: red"}

```{r}
### Validate 'gene_accession_id'

invalid_gene_lengths <- analysis %>% 
  filter(nchar(gene_accession_id) < 9 | nchar(gene_accession_id) > 11)

log_message(if (nrow(invalid_gene_lengths) == 0) "All gene_accession_id values are valid." else paste("Invalid gene_accession_id lengths:", nrow(invalid_gene_lengths)))
```

Check & format `gene_symbol` so all values are in title format.

```{r}
### Validate & format 'gene_symbol'

# Convert all gene_symbols to title format
analysis <- analysis %>%   mutate(
    gene_symbol = str_to_title(gene_symbol))  %>% 
  distinct()

# Check length of gene_symbol values
invalid_gene_symbols <- analysis %>% 
  filter(nchar(gene_symbol) < 1 | nchar(gene_symbol) > 13)

log_message(if (nrow(invalid_gene_symbols) == 0) "All gene_symbol values are valid." else paste("Invalid gene_symbol values:", nrow(invalid_gene_symbols)))
```

Check if `gene_accession_id` and `gene_symbol` are perfectly correlated.

```{r}
### Check `gene_accession_id` and `gene_symbol` correlation
gene_symbol_correlation <- analysis %>%
  group_by(gene_accession_id) %>%
  filter(n_distinct(gene_symbol) > 1)

log_message(if (nrow(gene_symbol_correlation) == 0) "Perfect correlation between gene_accession_id and gene_symbol." else "Correlation issues detected.")
```

Check & format `mouse_strain` so all results are one of the four valid strains specified in the SOP: C57BL, B6J, C3H or 129SV. Our results show 11 unique strains, 9 of which appear to be typos of C57BL (e.g. C53BL, C58BL - none of the 'typos' are valid mouse strains according to MGI).

```{r}
#### Validate and fix `mouse_strain`
analysis <- analysis %>%
  mutate(mouse_strain = ifelse(mouse_strain %in% c("C57BL", "B6J", "C3H", "129SV"), mouse_strain, "C57BL"))

log_message("Updated mouse_strain values.") # Selected 'C57BL' after looking at unique results - there appear to be typos of C57BL
```

Check if `mouse_life_stage` values are one of the allowed values according to the SOP.

```{r}
### Validate 'mouse_life_stage'
invalid_life_stages <- analysis %>% filter(!mouse_life_stage %in% c("E12.5", "E15.5", "E18.5", "E9.5", "Early adult", "Late adult", "Middle aged adult"))

log_message(if (nrow(invalid_life_stages) == 0) "All mouse_life_stage values are valid." else paste("Invalid mouse_life_stage values:", nrow(invalid_life_stages)))
```

Checking if `parameter_id` & `parameter_name` are the correct lengths (15-18 characters & 2-74 characters respectively). Also checking if they are perfectly correlated with each other.

```{r}
### Validate 'parameter_id'
invalid_parameter_ids <- analysis %>%
  filter(nchar(parameter_id) < 15 | nchar(parameter_id) > 18)

log_message(if (nrow(invalid_parameter_ids) == 0) {"All parameter_id values are valid."} else {paste("Invalid parameter_id values:", nrow(invalid_parameter_ids))})

### Validate 'parameter_name'
invalid_parameter_names <- analysis %>% 
  filter(nchar(parameter_name) < 2 | nchar(parameter_name) > 74)

log_message(if (nrow(invalid_parameter_names) == 0) {"All parameter_name values have valid lengths."} else {paste("Invalid parameter_name values:", nrow(invalid_parameter_names))})

### Correlation between 'parameter_id' & 'parameter_name'
parameter_correlation <- analysis %>% 
  group_by(parameter_id) %>% summarise(unique_names = n_distinct(parameter_name)) %>% 
  filter(unique_names > 1)

log_message(if (nrow(parameter_correlation) == 0) {"Perfect correlation between parameter_id and parameter_name."} else {"Correlation issues detected for parameter_id and parameter_name."})

```

Check if all `pvalues` are between 0 & 1, if not cap the values at those extremes. In this case, we had no values below 0 but we did have values above 1.

```{r}
### Validate & fix 'pvalue'

analysis <- analysis %>%
  mutate(pvalue = ifelse(pvalue > 1, 1, pvalue)) # Viewing pvalues before transforming shows values >1
invalid_pvalues <- analysis %>% filter(pvalue < 0 | pvalue > 1 | is.na(pvalue))

log_message(if (nrow(invalid_pvalues) == 0) "All pvalue values are valid." else paste("Invalid pvalue values:", nrow(invalid_pvalues)))
```

Save the cleaned dataframe to a new file.

```{r}
### SAVE CLEANED DATA ###
write_csv(analysis, paste0(rootDir, "data/cleaned/cleaned_analysis_table.txt"))

log_message("Cleaned dataset saved as 'cleaned_analysis_table.txt'.")
log_message("=== End of Log ===")
```

Log file shows the all the messaged corresponding to the if/else statement results in every step of this workflow.

```{r}
rm(invalid_gene_lengths, invalid_gene_symbols, invalid_ids, invalid_life_stages, invalid_parameter_ids, invalid_parameter_names, invalid_pvalues, parameter_correlation, type_mismatches, gene_symbol_correlation, log_file, log_message, updated_data_dir, SOP_path)
```

## Cleaning the Metadata Files

The cleaning of the procedure, parameter, and disease files involved two steps:

1.  **Initial Processing**: Fields were clearly separated by commas, and fields containing commas were enclosed in quotes to ensure proper loading as a table. A temporary text file reflecting these changes was saved.

2.  **Data Cleaning**: The temporary text file was loaded as a table, and values were cleaned to remove duplicates and ensure consistent formatting.

After these changes were applied, a quick check was performed to ensure the fields met the SOP requirements.

### `IMPC_procedure.txt`

#### Formatting

```{r}
# Define the path to temporarily store the formatted table before cleaning
temporary_path <- paste0(rootDir, "data/cleaned/IMPC_procedure_table.txt")

# Reformat the procedures file for proper table loading
procedure <- read_lines(paste0(rootDir, "data/metadata/IMPC_procedure.txt")) %>%
  str_replace_all('" "', ",") %>%           # Replace double quotes between fields with commas
  str_remove_all('^"|"$') %>%               # Remove leading and trailing double quotes from each line
  str_replace(                              # Wrap descriptions containing commas in quotes
    '^([0-9]+,[^,]+,)(.*)(, (TRUE|FALSE), [0-9]{5})$', 
    '\\1"\\2"\\3'
  ) %>%
  str_remove('line_number,') %>%            # Remove the "line_number" field name
  write_lines(temporary_path)               # Save the reformatted lines to the temporary file
```

#### Cleaning

```{r}
# Clean and standardize the procedure information
procedure <- read_csv(temporary_path, show_col_types = FALSE, col_names = TRUE) %>%
  
  # Apply basic cleaning steps
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%                   # Trim leading and trailing whitespace
      str_replace_all(c(               # Replace HTML entity codes
        "&amp;" = "&",                 
        "&nbsp;" = " ")) %>%           
      na_if("")                        # Replace empty strings with NA
  )) %>% 
  
  # Rename columns for consistency throughout 
  rename(
    procedure_id = procedureId,                       
    procedure_name = name,
    procedure_description = description,
    is_mandatory = isMandatory,
    parameter_mapping = impcParameterOrigId
  ) %>% 
  mutate(
    # Convert mapping to numeric, introduces NA for invalid entries  
    parameter_mapping = as.numeric(parameter_mapping),  
  # Standardize conflicting descriptions for rows referring to the same observation
    procedure_description = case_when(
      procedure_name == "Experimental design" ~ "experimental_design",        
      procedure_name == "Housing and Husbandry" ~ "housing_and_husbandry",
      procedure_name == "Viability Primary Screen" ~ "Assess the viability of mutant mice of each sex and zygosity.",
      procedure_name == "Electrocardiogram (ECG)" ~ "To provide a high throughput method to obtain Electrocardiograms in a conscious mouse.",
      is.na(procedure_description) ~ tolower(str_replace_all(procedure_name, " ", "_")), # Fill missing descriptions
      TRUE ~ procedure_description
    )
  ) %>% 
  
  # Group procedures to assign unique procedure IDs
  group_by(procedure_name, is_mandatory, procedure_description) %>%
  summarise(
    procedure_id = cur_group_id(),               
    parameter_mapping = list(parameter_mapping),# Store original IDs as a list for mapping with parameters
    .groups = "drop"                             
  ) %>% 

  # Expand the list of parameter mappings into individual rows
 unnest(parameter_mapping)

# Save the cleaned data to a new CSV file
write_csv(procedure, paste0(rootDir, "data/cleaned/cleaned_IMPC_procedure.csv"))
```

The table initially contained over 5,000 rows but only 50 distinct procedures, as each row represented a parameter tested within a procedure. To reduce redundancy:

-   Identical procedures with missing or conflicting descriptions were standardized by filling in or selecting the most appropriate value.

-   Procedures with identical names, descriptions, and mandatory values were assigned a shared `procedure_id`, with their `impcParameterOrigId` values stored as a list for integration with the parameter table.

IMPC link to procedures: <https://www.mousephenotype.org/impress/PipelineInfo?id=7>

```{r}
# Remove unnecessary files and variables
file.remove(temporary_path)
rm(temporary_path)
```

#### Validating

```{r}
# Validate procedure table for SOP compliance
errors <- c()

# Ensure procedure_name and procedure_description are strings
if (!all(sapply(procedure$procedure_name, is.character))) {
  errors <- c(errors, "'procedure_name' contains non-string values.")
}
if (!all(sapply(procedure$procedure_description, is.character))) {
  errors <- c(errors, "'procedure_description' contains non-string values.")
}

# Ensure is_mandatory contains only TRUE or FALSE
if (!all(procedure$is_mandatory %in% c(TRUE, FALSE))) {
  errors <- c(errors, "'is_mandatory' contains invalid values (not TRUE/FALSE).")
}

# Ensure procedure_id is a unique numeric value
if (!all(is.numeric(procedure$procedure_id), na.rm = TRUE)) {
  errors <- c(errors, "'procedure_id' contains non-numeric values.")
}
# Note: this only works when the parameter_mapping field is nested
if (length(unique(procedure$procedure_id)) != nrow(procedure)) {
  errors <- c(errors, "'procedure_id' contains duplicate values. This is normal if the parameter_mapping field is unnested.")
}

# Ensure parameter_mapping is numeric
if (!all(is.numeric(procedure$parameter_mapping), na.rm = TRUE)) {
  errors <- c(errors, "'parameter_mapping' contains non-numeric values. This is normal if the parameter_mapping field is nested.")
}

# Output validation results
if (length(errors) == 0) {
  print("All checks passed successfully.")
} else {
  print(paste("Errors found:", paste(errors, collapse = " ")))
}
```

### `IMPC_parameter_description.txt`

#### Formatting

```{r}
# Define the path to temporarily store the formatted table before cleaning
temporary_path <- paste0(rootDir, "data/cleaned/IMPC_parameter_description_table.txt")


# Reformat the parameters file for proper table loading
parameter <- read_lines(paste0(rootDir, "data/metadata/IMPC_parameter_description.txt")) %>%
  
  .[-1] %>%                                   # Remove the first line (header 'x')
  discard(~ . == "") %>%                      # Remove empty lines
  str_remove_all('^"|"$') %>%                 # Remove leading and trailing double quotes
  str_replace_all('" "', ",") %>%             # Replace double quotes separating fields with commas
  
  # Handle inconsistent newline characters
  str_c(collapse = "\n") %>%                  # Combine all lines into a single string
  str_replace_all("\n", " ") %>%              # Replace all existing newline characters with spaces
  # Add newline characters at the correct locations
  str_replace_all(c(
    '(IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})' = '\\1\n',  # Add a newline after each gene identifier (single observation)
    'line_number,impcParameterOrigId, name, description, parameterId' = 
      'parameter_mapping, parameter_name, parameter_description, parameter_id\n'  # Standardize header row and add a newline
  )) %>%
  str_split("\n") %>%                         # Split the string into lines based on the newline character
  unlist() %>%                                # Convert the resulting list into a flat character vector
  
  str_trim() %>%                              # Trim whitespace from the start and end of each line
  
  # Replace commas within parentheses with semicolons to avoid misinterpretation as field separators
  str_replace_all('\\(([^)]*)\\)', function(x) str_replace_all(x, ',', ';')) %>%
  
  # Format table rows by adding quotes around fields
  str_replace_all(
    '^([1-9][0-9]{0,3}),([0-9]{4,5}), ?([^,]+), ?(.*), ?(IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})$', 
    '"\\2","\\3","\\4","\\5"'
  ) %>%
  
  # Replace semicolons within parentheses back to commas
  str_replace_all('\\(([^)]*)\\)', function(x) str_replace_all(x, ';', ',')) %>%
  
  # Save the cleaned and standardized data to a new file
  write_lines(temporary_path)
```

#### Cleaning

```{r}
# Load and clean parameter data
parameter <- read_csv(temporary_path, show_col_types = FALSE) %>%
  
  # Clean all table cells: trim whitespace and replace empty strings with NA
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%   
      na_if("")        
  )) %>%  
  mutate(
    # Convert mapping to numeric, introduces NA for invalid entries  
    parameter_mapping = as.numeric(parameter_mapping),  
    # Update inconsistent parameter names
    parameter_name = case_when(
      # Experimental design procedure
      parameter_id == "IMPC_EXD_007_001" ~ "Frequency of controls",
      parameter_id == "IMPC_EXD_008_001" ~ "Number male controls",
      parameter_id == "IMPC_EXD_009_001" ~ "Number female controls",
      parameter_id == "IMPC_EXD_014_001" ~ "Core stock strategy",
      parameter_id == "IMPC_EXD_015_001" ~ "Knockout batch strategy",
      # Histopathology procedure
      parameter_id == "IMPC_HIS_008_001" ~ "Eye - Free text diagnostic term",
      parameter_id == "IMPC_HIS_010_001" ~ "Eye - Severity score",
      parameter_id == "IMPC_HIS_011_001" ~ "Eye - Descriptor PATO",
      parameter_id == "IMPC_HIS_012_001" ~ "Eye - Description",
      parameter_id == "IMPC_HIS_140_001" ~ "Testis - Free text diagnostic term",
      parameter_id == "IMPC_HIS_142_001" ~ "Testis - Severity score",
      parameter_id == "IMPC_HIS_143_001" ~ "Testis - Descriptor PATO",
      parameter_id == "IMPC_HIS_144_001" ~ "Testis - Description",
      parameter_id == "IMPC_HIS_152_001" ~ "Prostate gland - Free text diagnostic term",
      parameter_id == "IMPC_HIS_154_001" ~ "Prostate gland - Severity score",
      parameter_id == "IMPC_HIS_155_001" ~ "Prostate gland - Descriptor PATO",
      parameter_id == "IMPC_HIS_156_001" ~ "Prostate gland - Description",
      parameter_id == "IMPC_HIS_180_002" ~ "Eye - Significance score",
      parameter_id == "IMPC_HIS_202_002" ~ "Testis - Significance score",
      parameter_id == "IMPC_HIS_204_002" ~ "Prostate gland - Significance score",
      parameter_id == "IMPC_HIS_259_001" ~ "Trigeminal ganglion - Diagnostic term",
      parameter_id == "IMPC_HIS_261_001" ~ "Trigeminal ganglion - Severity score",
      parameter_id == "IMPC_HIS_262_001" ~ "Trigeminal ganglion - Descriptor PATO",
      parameter_id == "IMPC_HIS_263_001" ~ "Trigeminal ganglion - Description",
      parameter_id == "IMPC_HIS_264_001" ~ "Trigeminal ganglion - Significance score",
      # Housing and Husbandry procedure
  parameter_id == "IMPC_HOU_042_001" ~ "Nutrition - Diet Mass Known", # Nutrition - do you know the composition of the diet (average based on mass)

  parameter_id == "IMPC_HOU_046_001" ~ "Nutrition - Diet Cal Known", # Nutrition - do you know the composition of the diet (average based on calorific content)
  parameter_id == "IMPC_HOU_047_001" ~ "Nutrition - Diet Cal Carb", # Nutrition - average composition (based on calorific content) - Carbohydrate
  parameter_id == "IMPC_HOU_064_001" ~ "Microbiological Status - Pathogen Positive", # Microbiological status - is your unit positive for any of the pathogens tested
      TRUE ~ parameter_name  # Default case
    ),
    # Fill or update parameter descriptions
    parameter_description = case_when(
      parameter_id == "IMPC_EXD_015_001" ~ "Knockout batch strategy",
      parameter_id == "IMPC_HIS_008_001" ~ "Eye - Free text diagnostic term",
      parameter_id == "IMPC_HIS_010_001" ~ "Eye - Severity score",
      is.na(parameter_description) ~ tolower(str_replace_all(parameter_name, " ", "_")), # Fill missing descriptions with underscored name 
      TRUE ~ parameter_description  # Default case
    )
  ) %>%
  
  # Ensure one record per parameter
  distinct(parameter_name, parameter_description, parameter_id, .keep_all = TRUE)  # Reduces rows from 5325 to 3600; ignore parameter_mappings for this as each parameter is part of a specific procedure 

# Save the cleaned data to a new CSV file
write_csv(parameter, paste0(rootDir, "data/cleaned/cleaned_IMPC_parameter_description.csv"))
```

```{r}
# Remove unnecessary files and variables
file.remove(temporary_path)
rm(temporary_path)
```

#### Validating

```{r}
# Check the parameter table for SOP compliance
errors <- c()

# Check if parameter_mapping is numeric
if (!all(is.numeric(parameter$parameter_mapping), na.rm = TRUE)) {
  errors <- c(errors, "Some 'parameter_mapping' values are not numeric.")
}

# Check if parameter_name, parameter_description, and parameter_id are strings
if (!all(sapply(parameter$parameter_name, is.character))) {
  errors <- c(errors, "Some 'parameter_name' values are not strings.")
}
if (!all(sapply(parameter$parameter_description, is.character))) {
  errors <- c(errors, "Some 'parameter_description' values are not strings.")
}
if (!all(sapply(parameter$parameter_id, is.character))) {
  errors <- c(errors, "Some 'parameter_id' values are not strings.")
}

# Check if parameter_id are unique
if (length(unique(parameter$parameter_id)) != nrow(parameter)) {
  errors <- c(errors, "Duplicate 'parameter_id' values found.")
}

# Check if parameter_id and parameter_name length are within SOP constraints 
if (!all(nchar(parameter$parameter_id) >= 15 & nchar(parameter$parameter_id) <= 18, na.rm = TRUE)) {
  errors <- c(errors, "Some 'parameter_id' lengths are outside SOP constraints (15-18 characters).")
}
if (!all(nchar(parameter$parameter_name) >= 2 & nchar(parameter$parameter_name) <= 74, na.rm = TRUE)) {
  errors <- c(errors, "Some 'parameter_name' lengths are outside SOP constraints (2-74 characters).")
}

# Print results
if (length(errors) == 0) {
  print("All checks passed successfully.")
} else {
  print(paste("Errors found:", paste(errors, collapse = " ")))
}
```

### `disease_information.txt`

#### Formatting

```{r}
temporary_path <- paste0(rootDir, "data/cleaned/Disease_information_table.txt")
  
disease <- read_lines(paste0(rootDir, "data/metadata/Disease_information.txt")) %>% 
  .[-1] %>%    # Remove the first line (header 'x')
  str_remove_all('^\\"\\d+\\"\\s') %>% # Remove  line numbers enclosed in quotes followed by a space
  str_remove_all('^"|"$') %>%     # Remove any leading or trailing quotes
  
  # Separate fields based on the regex 
  str_replace_all(
    '(OMIM|ORPHA|DECIPHER):(\\d+),\\s*(.+?),\\s*(MGI:\\d+),\\s*(\\d+)',
    '"\\1:\\2","\\3","\\4","\\5"') %>%

  # Save the cleaned and standardized data to a new file
  write_lines(temporary_path)
```

#### Cleaning

```{r}
# Load and clean parameter data
disease <- read_csv(temporary_path, show_col_types = FALSE) %>%
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%   
      na_if("")        
  )) %>%  
  mutate(
    # Convert safely to numeric
    phenodigm_score = as.numeric(phenodigm_score)
  )%>%
  # Remove duplicates
  distinct()

# Save the cleaned data to a new CSV file
write_csv(disease, paste0(rootDir, "data/cleaned/cleaned_disease_information.csv"))

## WE STILL NEED TO DISCUSS HOW WE WANT TO ADRESS THIS 
# Find rows with the same disease_id and gene_accession_id
duplicates <- disease %>%
  group_by(disease_id, gene_accession_id) %>%
  filter(n() > 1) %>%
  ungroup()

# Check if duplicates exist and display them
if (nrow(duplicates) > 0) {
  cat("Rows with the same disease_id and gene_accession_id:\n")
  print(duplicates)
} else {
  cat("No rows with duplicate disease_id and gene_accession_id found.\n")
}

```

```{r}
# Remove unnecessary files and variables
file.remove(temporary_path)
rm(temporary_path)
```

#### Validating

```{r}
# Validate disease table for SOP compliance
errors <- c()

# Ensure disease_id, disease_term, and gene_accession_id are strings
if (!all(sapply(disease$disease_id, is.character))) {
  errors <- c(errors, "'disease_id' contains non-string values.")
}
if (!all(sapply(disease$disease_term, is.character))) {
  errors <- c(errors, "'disease_term' contains non-string values.")
}
if (!all(sapply(disease$gene_accession_id, is.character))) {
  errors <- c(errors, "'gene_accession_id' contains non-string values.")
}

# Ensure phenodigm_score is a double
if (!all(is.double(disease$phenodigm_score), na.rm = TRUE)) {
  errors <- c(errors, "'phenodigm_score' contains non-double values.")
}

# Validate disease_id format (starts with ORPHA, OMIM, or DECIPHER)
if (!all(grepl("^(ORPHA|OMIM|DECIPHER):\\d+$", disease$disease_id))) {
  errors <- c(errors, "'disease_id' does not conform to the expected format (ORPHA, OMIM, or DECIPHER followed by a colon and digits).")
}

# Validate gene_accession_id format (starts with MGI and is 9 to 11 characters)
if (!all(grepl("^MGI:\\d{5,7}$", disease$gene_accession_id))) {
  errors <- c(errors, "'gene_accession_id' does not conform to the expected format (MGI: followed by 5 to 7 digits).")
}

# Output validation results
if (length(errors) == 0) {
  print("All checks passed successfully.")
} else {
  print(paste("Errors found:", paste(errors, collapse = " ")))
}
```

```{r}
rm(errors)
```

# Collating

### GroupingsTable

To reduce the parameter space the collaborator would like to start grouping parameters_table together based on phenotype test similarity and/or naming similarity. Design the database to reflect new groupings for weight,  images and brain parameters_table, plus at least 3 others of your choice (refer to below). 

In addition to the data provided, each group must  find at least 3 other parameter groupings from the parameter list and add this to the  database. We encourage you to find as many additional parameter groups as you can and  time permits, however only 3 more is compulsory. parameters_table

```{r}
# Create a data frame with IDs and names
groupings_table <- data.frame(group_id = 1:6,group_name = c("weight", "brain", "images", "cardiovascular", "eye", "neuro"))

# Save the table as CSV 
write_csv(groupings_table, paste0(rootDir, "/data/collated/GroupingsTable.csv"), quote = "all")
```

This step subsets the data from the metadata table according to the database schema decide dupon:

```{r}
# load tables to not have to run the rest of the script: 
procedure <- read_csv(paste0(rootDir, "data/cleaned/cleaned_IMPC_procedure.csv"), show_col_types = FALSE) 
analysis <- read_csv(paste0(rootDir, "data/cleaned/cleaned_analysis_table.txt"), show_col_types = FALSE) 
parameter <- read_csv(paste0(rootDir, "data/cleaned/cleaned_IMPC_parameter_description.csv"), show_col_types = FALSE)
disease <- read_csv(paste0(rootDir, "data/cleaned/cleaned_Disease_information.csv"), show_col_types = FALSE)

```

### Procedures

```{r}
procedures_table <- procedure %>%
  select(procedure_id, procedure_name, procedure_description, is_mandatory) %>%   mutate(is_mandatory = as.integer(is_mandatory))  %>% # needed for sql that needs 0 and 1 for booleans 

  # Select specific columns
  distinct() # Keep only unique rows (this is needed due to the parameter_mapping field)

# Save the table as CSV 
write_csv(procedures_table, paste0(rootDir, "/data/collated/Procedures.csv"), quote = "all")
```

### Genes

```{r}
genes_table <- analysis %>% select(gene_accession_id, gene_symbol) %>% # Select desired columns
  bind_rows(disease %>% 
      select(gene_accession_id) %>%
      filter(!gene_accession_id %in% analysis$gene_accession_id)) %>%
  mutate(# Convert to title case 
    gene_symbol = case_when(
      gene_accession_id == "MGI:98967" ~ "Wt",
      gene_accession_id == "MGI:96090" ~ "Asmt",
      gene_accession_id == "MGI:98266" ~ "Sord",
      gene_accession_id == "MGI:1928271" ~ "Malrd1",
      gene_accession_id == "MGI:109279" ~ "Nnt",
      gene_accession_id == "MGI:3665157" ~ "Rsxr",
      gene_accession_id == "MGI:1345961" ~ "Coro1a",
      gene_accession_id == "MGI:87868" ~ "Acads",
      gene_accession_id == "MGI:1351634" ~ "Abcc6",
      gene_accession_id == "MGI:104511" ~ "Tnfsf4",
      TRUE ~ gene_symbol)) %>% 
  distinct()

write_csv(genes_table, paste0(rootDir, "/data/collated/Genes.csv"), quote = "all")
```

### Analyses

```{r}
analyses_table <- analysis %>%
  select(analysis_id, gene_accession_id, mouse_strain, mouse_life_stage, parameter_id, pvalue)   %>%  
    distinct()

write_csv(analyses_table, paste0(rootDir, "/data/collated/Analyses.csv"), quote = "all")
```

### Parameters

```{r}
# Join parameter table with procedure table by the appropriate column
parameters_table <- left_join(parameter, procedure, by = join_by(parameter_mapping)) %>%
  # Add rows from analyses_table where parameter_id is not already in the parameter table
  bind_rows(
    analyses_table %>%
      select(parameter_id) %>%
      filter(!parameter_id %in% parameter$parameter_id)
  ) %>%
  distinct(parameter_id, parameter_name, parameter_description, procedure_id)

# Write the final table to a CSV file with all fields quoted
write_csv(parameters_table, paste0(rootDir, "/data/collated/Parameters.csv"), quote = "all")

```

### Diseases

```{r}
diseases_table <- disease %>%
  select(disease_id, disease_term) %>% 
  distinct()

write_csv(diseases_table, paste0(rootDir, "/data/collated/Diseases.csv"), quote = "all")
```

### PhenoDigm

```{r}
phenodigm_scores <- disease %>%
  select(disease_id, gene_accession_id, phenodigm_score) 

write_csv(phenodigm_scores, paste0(rootDir, "/data/collated/PhenodigmScores.csv"), quote = "all") 
```

### Groupings

```{r}
# Define a lookup table for group names and regex patterns
group_patterns <- tibble(
  group_name = c("weight", "images", "brain", "cardiovascular", "eye"),
  regex_pattern = c( # patterns to match specific parameter names
    "weight|density|fat|lean|BMI|BMC|mass|composition|bone|length|adipose|lipid|body size|size|growth|skeletal|area|volume",
    "image|screenshot|microscope|pdf|lacz|waveform|ogram|resolution|visual|pixel|scan|photo|imaging|bit depth|microscopy|contrast",
    "brain|cortex|hippo|thalamus|amygdala|lobe|cere|spinal|pituitary|striatum|hypothal|neural|neuronal|pons|medulla|ganglion|nerve|enceph|ventricle|choroid",
    "heart|cardiac|vascular|blood|vein|aort|arter|HR|stroke|ejection|QT|circulation|pulse|pressure|rate|systolic|diastolic|respiration|ECG|cardio|valve",
    "eye|retina|cornea|lens|eyelid|iris|optic|ocular|vision|acuity|photoreceptor|brightness|scheimpflug|nuclear|macula|pupil|color|refraction|sclera|vitreous"
  )
)

groupXparameter_table <- parameter %>%
  select(parameter_id, parameter_name) %>%  # Select relevant columns
  # Matches each parameter_name against regex_pattern, retrieves matching group_name values, and creates a group_name column with a list of matches for each parameter.
  mutate(group_name = map(parameter_name, ~ group_patterns$group_name[
    str_detect(.x, regex(group_patterns$regex_pattern, ignore_case = TRUE))
  ])) %>%
  unnest_longer(group_name) %>%  # Expand rows for each matched group
  left_join(groupings_table, by = "group_name") %>%   # Add group_id from groupings_table
  select(parameter_id, group_id) %>% # Select relevant columns
  distinct() # Remove duplicates

write_csv(groupXparameter_table, paste0(rootDir, "/data/collated/ParameterGroupings.csv"), quote = "all")
```

# MySQL Database

```{sql}
#| eval: false

-- Check if MySQL is running
ps -ef | grep mysql

-- Connect to MySQL
mysql -u root -p --local-infile=1
-- local-infile=1 enables data import from local files using LOAD DATA LOCAL INFILE.
```

## Create Database and Schemas

```{sql}
#| eval: false

-- Create the database
CREATE DATABASE IMPCDb;

-- Select the database
USE IMPCDb;
```

```{sql}
-- Create tables without foreign keys first

-- Table: Genes
CREATE TABLE Genes (
    gene_accession_id VARCHAR(11) PRIMARY KEY,   -- Unique ID for each gene, 9-11 alphanumeric characters as per SOP
    gene_symbol VARCHAR(13)                      -- Gene symbol, 1-13 alphanumeric characters as per SOP
) COMMENT = 'Stores gene identifiers and symbols. Referenced by Analyses and PhenodigmScores (one-to-many).';

-- Table: ProceduresTable (avoids reserved keyword "Procedure")
CREATE TABLE ProceduresTable (
    procedure_id INT PRIMARY KEY,                -- Unique ID for each procedure
    procedure_name VARCHAR(47),                  -- Procedure name, max 47 characters
    procedure_description VARCHAR(1090),         -- Procedure description, max 1090 characters
    is_mandatory BOOLEAN NOT NULL DEFAULT FALSE  -- Mandatory flag, TRUE/FALSE; defaults to FALSE
) COMMENT = 'Stores procedure details (name, description, mandatory flag). Linked to Parameters via a one-to-many relationship.';

-- Table: Diseases
CREATE TABLE Diseases (
    disease_id VARCHAR(50) PRIMARY KEY,          -- Unique ID for each disease, max 50 characters
    disease_term VARCHAR(150)                    -- Descriptive term for the disease, max 150 characters
) COMMENT = 'Stores disease identifiers and terms. Referenced in PhenodigmScores (many-to-many relationship with Genes).';

-- Table: Groupings
CREATE TABLE Groupings (
    group_id INT AUTO_INCREMENT PRIMARY KEY,     -- Auto-incrementing unique ID for each group
    group_name VARCHAR(50)                       -- Group name, max 50 characters
) COMMENT = 'Defines parameter groups. Linked to Parameters via a many-to-many relationship.';

-- Create tables with foreign keys

-- Table: Parameters
CREATE TABLE Parameters (
    parameter_id VARCHAR(18) PRIMARY KEY,          -- Unique ID for each parameter, 15-18 alphanumeric characters as per SOP
    parameter_name VARCHAR(74),                    -- Parameter name, 2-74 characters as per SOP
    parameter_description VARCHAR(1000),           -- Parameter description, max 1000 characters
    procedure_id INT,                              -- Foreign key to ProceduresTable
    FOREIGN KEY (procedure_id) REFERENCES ProceduresTable(procedure_id)
) COMMENT = 'Stores parameter metadata. References ProceduresTable (many-to-one) and referenced by Analyses (one-to-many).';

-- Table: Analyses
CREATE TABLE Analyses (
    analysis_id VARCHAR(15) PRIMARY KEY,           -- Unique ID for each analysis, 15 alphanumeric characters as per SOP
    gene_accession_id VARCHAR(11),                 -- Foreign key to Genes, 9-11 alphanumeric characters as per SOP
    mouse_strain VARCHAR(5),                       -- Mouse strain, 3-5 alphanumeric characters as per SOP
    mouse_life_stage VARCHAR(17),                  -- Mouse life stage, 4-17 characters as per SOP
    parameter_id VARCHAR(18),                      -- Foreign key to Parameters
    p_value FLOAT,                                 -- P-value, float between 0 and 1 as per SOP
    FOREIGN KEY (gene_accession_id) REFERENCES Genes(gene_accession_id),
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id)
) COMMENT = 'Stores analysis results linking Genes and Parameters (many-to-one), including p-values, mouse strain, and life stage.';

-- Table: PhenodigmScores
CREATE TABLE PhenodigmScores (
    phenodigm_id INT AUTO_INCREMENT PRIMARY KEY,   -- Auto-incrementing unique ID for each record
    disease_id VARCHAR(50),                        -- Foreign key to Diseases
    gene_accession_id VARCHAR(11),                 -- Foreign key to Genes
    phenodigm_score FLOAT,                         -- Phenodigm association score, float value
    FOREIGN KEY (disease_id) REFERENCES Diseases(disease_id),
    FOREIGN KEY (gene_accession_id) REFERENCES Genes(gene_accession_id)
) COMMENT = 'Stores gene-disease association scores, linking Genes and Diseases in a many-to-many relationship.';

-- Table: ParameterGroupings
CREATE TABLE ParameterGroupings (
    parameter_id VARCHAR(18),                      -- Foreign key to Parameters
    group_id INT,                                  -- Foreign key to Groupings
    PRIMARY KEY (parameter_id, group_id),          -- Composite primary key for the many-to-many relationship
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id),
    FOREIGN KEY (group_id) REFERENCES Groupings(group_id)
) COMMENT = 'Links Parameters and Groupings in a many-to-many relationship.';

# Consider adding indexes for frequently queried foreign key columns to improve performance.
```

## Populate Database

To populate the database, the schema should first be examined to identify which attributes correspond to each table. Data should then be organized into separate CSV or TSV files, with each file representing a single table. Each file is expected to include a header row (optional but recommended) that matches the column names defined in the database schema.

Once a CSV file has been created for each table, the data can be loaded directly into MySQL using the `LOAD DATA INFILE` command.

```{sql}
-- Load data into tables using the base path
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Procedures.csv'
INTO TABLE ProceduresTable
FIELDS TERMINATED BY ','      -- Fields separated by commas
OPTIONALLY ENCLOSED BY '"'    -- Fields (optionally) enclosed in double quotes
IGNORE 1 LINES;               -- Skip the header row in the CSV file

SHOW WARNINGS;

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Genes.csv'
INTO TABLE Genes
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;

SHOW WARNINGS;

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Groupings.csv'
INTO TABLE Groupings
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;

SHOW WARNINGS;

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Parameters.csv'
INTO TABLE Parameters
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;

SHOW WARNINGS;

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Diseases.csv'
INTO TABLE Diseases
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;

SHOW WARNINGS;

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/PhenodigmScores.csv'
INTO TABLE PhenodigmScores
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES
(disease_id, gene_accession_id, phenodigm_score); -- Explicitly map columns (phenodigm_id is auto-generated)

SHOW WARNINGS;

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Analyses.csv'
INTO TABLE Analyses
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;

SHOW WARNINGS;

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/ParameterGroupings.csv'
INTO TABLE ParameterGroupings
FIELDS TERMINATED BY ','               
OPTIONALLY ENCLOSED BY '"'                   
IGNORE 1 LINES;  

SHOW WARNINGS;
```

##### A total of 143 parameters corresponding to 83,712 analysis files have non-IMPC identifiers, meaning they are not linked to any procedure. This violates the foreign key constraints in the database, resulting in the following issues: {style="color: red"}

1.  During the loading of the `Parameters` table, missing or invalid `procedure_id` values (e.g., `NA`) cause errors, such as "Incorrect integer value: 'NA' for column 'procedure_id' at row 3601." Additionally, foreign key violations occur when `procedure_id` values are not linked to `ProceduresTable`.

2.  During the loading of the `Analyses` table, non-IMPC `parameter_id` values prevent rows from referencing the `Parameters` table, resulting in foreign key constraint failures.

### Create dump

```{bash}
#| eval: false
/opt/homebrew/bin/mysqldump IMPCDb -u root -p > /Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/database/IMPCDb.dump
```

# R/Shiny

**Test R/Shiny 1:** They want the ability to select a particular knockout mouse and visualise the statistical scores of all phenotypes tested.

```{sql}
-- Select p-values and corresponding parameter names for specific genes
SELECT 
    Analyses.p_value, 
    Parameters.parameter_name
FROM 
    Analyses
JOIN 
    Parameters ON Analyses.parameter_id = Parameters.parameter_id
WHERE 
    Analyses.gene_accession_id IN (
        -- Subquery to find gene_accession_id for specific gene symbols
        SELECT 
            gene_accession_id 
        FROM 
            Genes 
        WHERE 
            gene_symbol IN ('Ifi204', 'Mettl21c', 'Ier5', 'Abca4')
    )
ORDER BY 
    Analyses.p_value ASC; -- Sort results by p_value in ascending order

```

![](images/Screenshot%202024-12-21%20at%2016.21.52.png)

**Test R/Shiny 2:** The collaborator also wishes to visualise the statistical scores of all knockout mice for a selected phenotype.

```{sql}
#| eval: false
SELECT 
    Analyses.p_value, 
    Genes.gene_symbol
FROM 
    Analyses
JOIN 
    Parameters ON Analyses.parameter_id = Parameters.parameter_id
JOIN 
    Genes ON Analyses.gene_accession_id = Genes.gene_accession_id
WHERE 
    Parameters.parameter_name IN (
        'Facial Cleft',
        'Lens Opacity',
        'Triglycerides',
        'Skin color - tail',
        'Potassium',
        'HRV',
        'Hindbrain morphology',
        'Aspartate aminotransferase',
        'Tibia length',
        'Left total retinal thickness',
        'Embryo Size',
        'Effector CD4+ T helper cells - % of live leukocytes (Panel A)',
        'Limb morphology',
        'Coat - color - back',
        'Aggression',
        'Joints'
    )
ORDER BY 
    Analyses.p_value ASC;
```

![](images/Screenshot%202024-12-21%20at%2016.22.09.png)

**Test R/Shiny 3:** The collaborator would like to visualise clusters of genes with similar phenotype scores.  

```{sql}
#| eval: false

SELECT Genes.gene_symbol, Parameters.parameter_name, Analyses.p_value FROM Analyses JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id JOIN Genes ON Analyses.gene_accession_id = Genes.gene_accession_id; 
```

![](images/Screenshot%202024-12-21%20at%2016.22.33.png)
