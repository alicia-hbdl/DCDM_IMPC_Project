---
title: "DCDM Project Log"
author: "Alicia"
editor: visual
format: html
---

# TRE Egress (Ella)

## **Data Security Awareness Training**

1.  Account created through the [NHS e-Learning for Healthcare portal](https://portal.e-lfh.org.uk/Component/Details/544034)
2.  Course *Data Security Awareness - Level 1* :
    -   Completed the training and passed the examination

## **CREATE TRE Egress**

1.  Logged on to the Virtual Machine
    -   This [link](https://portal.er.kcl.ac.uk/vdiresource/281) was used to access a virtual machine (specific machine: er-tre-352-vm07) through the KCL e-research portal er_tre_msc_dcdm Virtual Desktops
    -   Logged into Virtual Machine with my KCL login
2.  From the Desktop, clicked on the "Data_Egress_Request” icon, which lead to the KCL TRE Data Egress Portal
3.  Clicked on the "Egress request tab", which led to a form to fill out with details about the data requested
    -   Selected "Group3.zip" within the file picker, requesting our specific dataset
    -   Answered questions about the classification, point of origin, end-point destination, purpose for egress, and type of data, specific to our dataset
4.  Submitted the Egress Request
5.  The egress was approved, and the data was made available to us as a zip file (Group3.zip) on the CREATE HPC in the /scratch_tmp/grp/msc_appbio/7BBG1003/CW folder

## Moving the Data

Though we had access to the zip file of our data, we had no permissions on it so were unable to move or copy it to our own data. This required me to download the file to my own system using scp, then re-upload the file to a our group project folder using scp. I checked the file sizes before and after using the `ls -l` command, to ensure no data was corrupted in the process.

```{bash}
# downloading the data locally from the HPC
# scp -i ~/.ssh/msckey  k24064085@hpc.create.kcl.ac.uk:/scratch_tmp/grp/msc_appbio/7BBG1003/CW/Group3.zip ~/Desktop/applied_bioinformatics/DCDM/group_project/raw_data/Group3

# uploading the data into our DCDM_group3 directory on the HPC
# scp -i ~/.ssh/msckey  ~/Desktop/applied_bioinformatics/DCDM/group_project/raw_data/Group3.zip k24064085@hpc.create.kcl.ac.uk:/sratch_tmp/grp/msc_appbio/DCDM_group3
```

# Data Cleaning

```{r}
rm(list = ls())

# Install and load Tidyverse packages
#install.packages(c("tidyverse", "janitor"))
library(tidyverse)
library(janitor)
```

```{r}
# Define the working directory path in r
# Alicia
rootDir = "/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
# Sanj 
#rootDir = "/Users/sanjanasrinivasan/Desktop/DCDM_IMPC_Project/"
# Ella
# rootDir = "/Users/ellajadealex/Desktop/applied_bioinformatics/DCDM/group_project/DCDM_IMPC_Project/"
# Sama
#rootDir = "/Users/sama/Desktop/DCDM_Proj2/"
# Esta
#rootDir = "~/Desktop/working_directory/DCDM_project"
```

## Cleaning the Raw Data Files

### Quality Control/Cleaning Using the SOP (Esta)

The experiment data includes an SOP that defines constraints for each field, such as data type, minimum and maximum values for numerical fields, string lengths, and specific allowable values. Data quality is assessed by comparing the raw data against these constraints.

In this step, we have cleaned all CSV files by modifying the row names to match those specified in the SOP.

#### Setting-Up the Environment

We begin by defining the directory paths and the log file path.

```{r}
# Defing directory paths
raw_data_dir <- paste0(rootDir, "/data/raw_data") # Input raw data directory
updated_data_dir <- paste0(rootDir, "/data/updated_data") # Cleaned analysis data directory (field names only)
log_file_mod <- paste0(rootDir, "/scripts/row_name_modification.log") # Log file for modifications
log_file_valid <- paste0(rootDir, "/scripts/row_name_validation.log")   # Log file for results
SOP_path <- paste0(rootDir, "/data/metadata/IMPC_SOP.csv") # SOP path

# Creates output directory if it doesn't already exist & supresses warnings if it does exist
dir.create(updated_data_dir, showWarnings = FALSE)

# Clear the log files if they exist or create them otherwise
file.create(log_file_mod)
file.create(log_file_valid)
```

Then we load the SOP file. *Here, I have normalized the row names to lowercase although this isn't strictly necessary for this specific file.*

```{r}
# Load the SOP file and normalize row names
expected_row_names <- trimws(read_csv(SOP_path, show_col_types = FALSE)$dataField)

# Check normalized row names
expected_row_names
```

#### Modify the field names

With the correct files loaded and directories specified, we define a function to perform the following tasks:

1.  Identify and correct spelling mistakes in the row names. In this case, only uppercase-to-lowercase transformations are needed;

2.  Add any missing rows, assigning `NA` to the second column for those rows. This step is performed after correcting any misspelled row names;

3.  Reorder the rows to align with the SOP layout.

```{r}
# Function to validate, update, and reorder row names
process_file <- function(file_path) {
  # Read the CSV file as headerless
  data <- tryCatch(
    read_csv(file_path, col_names = FALSE, show_col_types = FALSE), 
    error = function(e) return(NULL))
  
  # If file reading fails, skip processing
  if (is.null(data)) return(FALSE)
  
  # Normalize row names
  data[[1]] <- tolower(data[[1]])
  
  # Identify missing rows and add them, input NA value for second column
  missing_rows <- setdiff(expected_row_names, data[[1]])
  if (length(missing_rows) > 0) {
    missing_data <- tibble(!!colnames(data)[1] := missing_rows)
    for (i in 2:ncol(data)) {
      missing_data[[colnames(data)[i]]] <- NA
    }
    data <- bind_rows(data, missing_data)
  }
  
  # Reorder rows based on expected_row_names
  data <- arrange(data, match(data[[1]], expected_row_names))
  
  # Save the updated file as headerless
  write_csv(data, file.path(updated_data_dir, basename(file_path)), col_names = FALSE)
 
  return(TRUE)
}
```

We now apply this function to all data files. This process will generate an `updated_data` folder containing the cleaned files and a log file in the main directory.

```{r}
# Process files and log results
validation_results <- sapply(list.files(raw_data_dir, pattern = "\\.csv$", full.names = TRUE), process_file)

# Log summary
summary_message <- sprintf("Validation Summary:\nTotal files checked: %d\nTotal updated files: %d\n",
                           length(validation_results), sum(validation_results))
write(summary_message, log_file_mod)
message(summary_message)
```

The summary message displays the number of files validated out of the total number in the `raw_data` folder. This information is shown on the console and saved to the log file. The log file, now streamlined to include only this summary, initially contained detailed row name differences between the files and the SOP, based on a subset of 600 files.

#### Verify the field names

#### This code duplicates the logic used to modify the files, making it somewhat redundant and unable to detect field name issues. I have kept it as a separate optional script to check for any potential collating errors - this script is a remnant of the data exploration code I used to identify the steps needed during data cleaning. {style="color: red"}

After modifying the files, we run a final validation script to confirm that there are no unexpected row name differences remaining. This ensures the data aligns perfectly with the SOP.

```{r}
#| eval: false

# Function to validate a file against the SOP layout and column count
validate_file <- function(file_path) {
  # Read the file
  data <- tryCatch(
    read_csv(file_path, col_names = FALSE, show_col_types = FALSE),
    error = function(e) return(NULL))
  
  # Skip files that could not be read
  if (is.null(data)) return(FALSE)
  
  # Check column count
  if (ncol(data) != 2) {
    return(list(valid = FALSE, message = "Incorrect column count"))
  }
  
  # Normalize row names and validate against SOP
  row_names <- tolower(data[[1]])
  missing_rows <- setdiff(expected_row_names, row_names)
  extra_rows <- setdiff(row_names, expected_row_names)
  
  # Create validation message
  if (length(missing_rows) > 0 || length(extra_rows) > 0) {
    message <- c(
      if (length(missing_rows) > 0) paste("Missing row names:", paste(missing_rows, collapse = ", ")),
      if (length(extra_rows) > 0) paste("Extra row names:", paste(extra_rows, collapse = ", "))
    )
    return(list(valid = FALSE, message = paste(message, collapse = "\n")))
  }
  
  list(valid = TRUE, message = "File is valid")
}

# Process all files
validation_results <- lapply(list.files(updated_data_dir, pattern = "\\.csv$", full.names = TRUE), function(file_path) {
  result <- validate_file(file_path)
  log_message <- paste(basename(file_path), ":", result$message, "\n")
  cat(log_message, file = log_file, append = TRUE)
  result$valid
})

# Summarize and log results
valid_files <- sum(unlist(validation_results))
summary_message <- sprintf(
  "Validation Summary:\nTotal files checked: %d\nTotal valid files: %d\n", 
  length(validation_results), valid_files
)
cat(summary_message, file = log_file_valid, append = TRUE)
message(summary_message)

```

This code can be run both locally and on an HPC with minimal adjustments, such as loading individual `dplyr` and `readr` packages instead of the entire `tidyverse`.

To check the differences between the raw and cleaned files (quick visual check to see if we are doing something).

```{bash}
#| eval: false

diff -yr --suppress-common-line /path/to/raw_directory /path/to/cleaned_directory
```

```{r}
# Remove unused values to unclutter the environment
rm(log_file_mod, log_file_valid, raw_data_dir, updated_data_dir, valid_files, total_files, validation_results, files, summary_message, expected_row_names)
```

### Combining the Raw Data in a Table

The dataset consists of thousands of CSV files with key fields: `analysis_id`, `gene_accession_id`, `gene_symbol`, `mouse_strain`, `mouse_life_stage`, `parameter_id`, `parameter_name`, and `pvalue`.

To streamline data cleaning and enable further analysis, these files must be combined into a single table based on their shared fields.

Below is a function that reads a file and converts its data into a row for the unified table:

```{r}
#| eval: false

# Define a function to process a single file
process_file <- function(file_path) {
  read.csv(file_path, header = FALSE, col.names = c("field", "value")) %>%
    # Transpose the data so that fields become column names and values form the row
    t() %>% 
    # Convert to dataframe (needed by the map_dfr function)
    as.data.frame() %>% 
    # Set the first row (after transposing) as the column names
    setNames(.[1, ]) %>%
    # Remove the first row since it is now used as column names
    slice(-1) 
}

# List all CSV files in the updated_data directory
all_files <- list.files(path = updated_data_dir, pattern = "\\.csv$", full.names = TRUE) 

# Apply the function to each file and combine the results into a single data frame
analysis_table <- map_dfr(all_files, process_file)

# Specify the output path
output_path <- file.path(rootDir, "data/cleaned/analysis_table.txt")

# Save the combined table to a text file
write.table(analysis_table, file = output_path, sep = ",", quote = TRUE, row.names = FALSE)

# Display the first 6 rows of the combined table
head(analysis_table)
```

### Cleaning the Collated Raw Data (Esta)

For each column, we need to check similar things, like whether column values are the right lengths (for strings) or size (for floats), and whether the results matche allowed values, like for `mouse_strain` and `mouse_life_stage`.We also need to check if certain variables are perfectly correlated with one another.

Check if variable data types match those specified in the SOP - all character (or string) data types except for pvalue (numeric or float).

```{r}
# Read the collated file
data <- read.csv("data/analysis_table.txt", header = TRUE, stringsAsFactors = FALSE)

# Show data types
print(sapply(data, class))
```

Check if `analysis_id` values are all 15 characters long and all unique.

```{r}
### Validate 'analysis_id'

invalid_ids <- data %>%
  filter(nchar(analysis_id) != 15 | duplicated(analysis_id))
message(if (nrow(invalid_ids) == 0) {
  "All analysis_id values are valid."
} else {
  paste("Invalid analysis_id values found:", nrow(invalid_ids))
})
```

Check if `gene_accession_id` values are all between 9-11 characters long - According to the SOP they should all be unique but our script shows over 190,000 duplicated results. We have chosen to consider duplicated gene_accession_ids valid.

```{r}
### Validate 'gene_accession_id'

invalid_gene_lengths <- data %>% filter(nchar(gene_accession_id) < 9 | nchar(gene_accession_id) > 11)
duplicate_genes <- data %>% group_by(gene_accession_id) %>% filter(n() > 1) %>% ungroup()
message(if (nrow(invalid_gene_lengths) == 0) "All gene_accession_id values are valid." else paste("Invalid gene_accession_id lengths:", nrow(invalid_gene_lengths)))
message(if (nrow(duplicate_genes) == 0) "All gene_accession_id values are unique." else paste("Duplicate gene_accession_id values:", nrow(duplicate_genes)))
```

Check & format `gene_symbol` so all values are in title format.

```{r}
### Validate & format 'gene_symbol'

to_title_format <- function(gene) paste0(toupper(substr(gene, 1, 1)), tolower(substr(gene, 2, nchar(gene))))
data <- data %>% mutate(gene_symbol = sapply(gene_symbol, to_title_format))
invalid_gene_symbols <- data %>% filter(nchar(gene_symbol) < 1 | nchar(gene_symbol) > 13)
message(if (nrow(invalid_gene_symbols) == 0) "All gene_symbol values are valid." else paste("Invalid gene_symbol values:", nrow(invalid_gene_symbols)))
```

Check if `gene_accession_id` and `gene_symbol` are perfectly correlated.

```{r}
### Check `gene_accession_id` and `gene_symbol` correlation

gene_symbol_correlation <- data %>%
  group_by(gene_accession_id) %>%
  filter(n_distinct(gene_symbol) > 1)
message(if (nrow(gene_symbol_correlation) == 0) "Perfect correlation between gene_accession_id and gene_symbol." else "Correlation issues detected.")
```

Check & format `mouse_strain` so all results are one of the four valid strains specified in the SOP: C57BL, B6J, C3H or 129SV. Our results show 11 unique strains, 9 of which appear to be typos of C57BL (e.g. C53BL, C58BL - none of the 'typos' are valid mouse strains according to MGI).

```{r}
#### Validate and fix `mouse_strain`

valid_strains <- c("C57BL", "B6J", "C3H", "129SV")
data <- data %>%
  mutate(mouse_strain = ifelse(mouse_strain %in% valid_strains, mouse_strain, "C57BL"))
message("Updated mouse_strain values.") # Selected 'C57BL' after looking at unique results - there appear to be typos of C57BL
```

Check if `mouse_life_stage` values are one of the allowed values according to the SOP.

```{r}
### Validate 'mouse_life_stage'

allowed_life_stages <- c("E12.5", "E15.5", "E18.5", "E9.5", "Early adult", "Late adult", "Middle aged adult")
invalid_life_stages <- data %>% filter(!mouse_life_stage %in% allowed_life_stages)
message(if (nrow(invalid_life_stages) == 0) "All mouse_life_stage values are valid." else paste("Invalid mouse_life_stage values:", nrow(invalid_life_stages)))
```

Checking if `parameter_id` is the correct length (15-18 characters) and is perfectly correlated with `parameter_name`. We chose not to validate `parameter_name` specifically as viewing this column shows no suspicious results.

```{r}
### Validate 'parameter_id' and 'parameter_name'

invalid_parameter_ids <- data %>% filter(nchar(parameter_id) < 15 | nchar(parameter_id) > 18)
parameter_correlation <- data %>%
  group_by(parameter_id) %>%
  summarise(unique_names = n_distinct(parameter_name)) %>%
  filter(unique_names > 1)
message(if (nrow(invalid_parameter_ids) == 0) "All parameter_id values are valid." else paste("Invalid parameter_id values:", nrow(invalid_parameter_ids)))
message(if (nrow(parameter_correlation) == 0) "Perfect correlation between parameter_id and parameter_name." else "Correlation issues detected for parameter_id and parameter_name.")
# Chose not to validate parameter_name separately 
```

Check if all `pvalues` are between 0 & 1, if not cap the values at those extremes. In this case, we had no values below 0 but we did have values above 1.

```{r}
### Validate & fix 'pvalue'

data <- data %>%
  mutate(pvalue = ifelse(pvalue > 1, 1, pvalue)) # Viewing pvalues before transforming shows values >1
invalid_pvalues <- data %>% filter(pvalue < 0 | pvalue > 1 | is.na(pvalue))
message(if (nrow(invalid_pvalues) == 0) "All pvalue values are valid." else paste("Invalid pvalue values:", nrow(invalid_pvalues)))
```

Save the cleaned dataframe to a new file.

```{r}
### SAVE CLEANED DATA ###

write.csv(data, "cleaned_analysis_table.txt", row.names = FALSE)
message("Cleaned dataset saved as 'cleaned_analysis_table.txt'.")
```

## Cleaning the Metadata Files

The cleaning of the procedure, parameter, and disease files involved two steps:

1.  **Initial Processing**: Fields were clearly separated by commas, and fields containing commas were enclosed in quotes to ensure proper loading as a table. A temporary text file reflecting these changes was saved.

2.  **Data Cleaning**: The temporary text file was loaded as a table, and values were cleaned to remove duplicates and ensure consistent formatting.

After these changes were applied, a quick check was performed to ensure the fields met the SOP requirements.

### `IMPC_procedure.txt`

#### Formatting

```{r}
# Define the path to temporarily store the formatted table before cleaning
temporary_path <- paste0(rootDir, "data/cleaned/IMPC_procedure_table.txt")

# Reformat the procedures file for proper table loading
procedure <- read_lines(paste0(rootDir, "data/metadata/IMPC_procedure.txt")) %>%
  str_replace_all('" "', ",") %>%           # Replace double quotes between fields with commas
  str_remove_all('^"|"$') %>%               # Remove leading and trailing double quotes from each line
  str_replace(                              # Wrap descriptions containing commas in quotes
    '^([0-9]+,[^,]+,)(.*)(, (TRUE|FALSE), [0-9]{5})$', 
    '\\1"\\2"\\3'
  ) %>%
  str_remove('line_number,') %>%            # Remove the "line_number" field name
  write_lines(temporary_path)               # Save the reformatted lines to the temporary file
```

#### Cleaning

```{r}
# Clean and standardize the procedure information
procedure <- read_csv(temporary_path, show_col_types = FALSE, col_names = TRUE) %>%
  
  # Apply basic cleaning steps
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%                   # Trim leading and trailing whitespace
      str_replace_all(c(               # Replace HTML entity codes
        "&amp;" = "&",                 
        "&nbsp;" = " ")) %>%           
      na_if("")                        # Replace empty strings with NA
  )) %>% 
  
  # Rename columns for consistency throughout 
  rename(
    procedure_id = procedureId,                       
    procedure_name = name,
    procedure_description = description,
    is_mandatory = isMandatory,
    parameter_mapping = impcParameterOrigId
  ) %>% 
  mutate(
    # Convert mapping to numeric, introduces NA for invalid entries  
    parameter_mapping = as.numeric(parameter_mapping),  
  # Standardize conflicting descriptions for rows referring to the same observation
    procedure_description = case_when(
      procedure_name == "Experimental design" ~ "experimental_design",        
      procedure_name == "Housing and Husbandry" ~ "housing_and_husbandry",
      procedure_name == "Viability Primary Screen" ~ "Assess the viability of mutant mice of each sex and zygosity.",
      procedure_name == "Electrocardiogram (ECG)" ~ "To provide a high throughput method to obtain Electrocardiograms in a conscious mouse.",
      is.na(procedure_description) ~ tolower(str_replace_all(procedure_name, " ", "_")), # Fill missing descriptions
      TRUE ~ procedure_description
    )
  ) %>% 
  
  # Group procedures to assign unique procedure IDs
  group_by(procedure_name, is_mandatory, procedure_description) %>%
  summarise(
    procedure_id = cur_group_id(),               
    parameter_mapping = list(parameter_mapping),# Store original IDs as a list for mapping with parameters
    .groups = "drop"                             
  ) #%>% 

  # Expand the list of parameter mappings into individual rows
 #unnest(parameter_mapping)

# Save the cleaned data to a new CSV file
write_csv(procedure, paste0(rootDir, "data/cleaned/cleaned_IMPC_procedure.csv"))
```

The table initially contained over 5,000 rows but only 50 distinct procedures, as each row represented a parameter tested within a procedure. To reduce redundancy:

-   Identical procedures with missing or conflicting descriptions were standardized by filling in or selecting the most appropriate value.

-   Procedures with identical names, descriptions, and mandatory values were assigned a shared `procedure_id`, with their `impcParameterOrigId` values stored as a list for integration with the parameter table.

IMPC link to procedures: <https://www.mousephenotype.org/impress/PipelineInfo?id=7>

```{r}
# Remove unnecessary files and variables
file.remove(temporary_path)
rm(temporary_path)
```

#### Validating

```{r}
# Validate procedure table for SOP compliance
errors <- c()

# Ensure procedure_name and procedure_description are strings
if (!all(sapply(procedure$procedure_name, is.character))) {
  errors <- c(errors, "'procedure_name' contains non-string values.")
}
if (!all(sapply(procedure$procedure_description, is.character))) {
  errors <- c(errors, "'procedure_description' contains non-string values.")
}

# Ensure is_mandatory contains only TRUE or FALSE
if (!all(procedure$is_mandatory %in% c(TRUE, FALSE))) {
  errors <- c(errors, "'is_mandatory' contains invalid values (not TRUE/FALSE).")
}

# Ensure procedure_id is a unique numeric value
if (!all(is.numeric(procedure$procedure_id), na.rm = TRUE)) {
  errors <- c(errors, "'procedure_id' contains non-numeric values.")
}
# Note: this only works when the parameter_mapping field is nested
if (length(unique(procedure$procedure_id)) != nrow(procedure)) {
  errors <- c(errors, "'procedure_id' contains duplicate values. This is normal if the parameter_mapping field is unnested.")
}

# Ensure parameter_mapping is numeric
if (!all(is.numeric(procedure$parameter_mapping), na.rm = TRUE)) {
  errors <- c(errors, "'parameter_mapping' contains non-numeric values. This is normal if the parameter_mapping field is nested.")
}

# Output validation results
if (length(errors) == 0) {
  print("All checks passed successfully.")
} else {
  print(paste("Errors found:", paste(errors, collapse = " ")))
}
```

### `IMPC_parameter_description.txt`

#### Formatting

```{r}
# Define the path to temporarily store the formatted table before cleaning
temporary_path <- paste0(rootDir, "data/cleaned/IMPC_parameter_description_table.txt")


# Reformat the parameters file for proper table loading
parameter <- read_lines(paste0(rootDir, "data/metadata/IMPC_parameter_description.txt")) %>%
  
  .[-1] %>%                                   # Remove the first line (header 'x')
  discard(~ . == "") %>%                      # Remove empty lines
  str_remove_all('^"|"$') %>%                 # Remove leading and trailing double quotes
  str_replace_all('" "', ",") %>%             # Replace double quotes separating fields with commas
  
  # Handle inconsistent newline characters
  str_c(collapse = "\n") %>%                  # Combine all lines into a single string
  str_replace_all("\n", " ") %>%              # Replace all existing newline characters with spaces
  # Add newline characters at the correct locations
  str_replace_all(c(
    '(IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})' = '\\1\n',  # Add a newline after each gene identifier (single observation)
    'line_number,impcParameterOrigId, name, description, parameterId' = 
      'parameter_mapping, parameter_name, parameter_description, parameter_id\n'  # Standardize header row and add a newline
  )) %>%
  str_split("\n") %>%                         # Split the string into lines based on the newline character
  unlist() %>%                                # Convert the resulting list into a flat character vector
  
  str_trim() %>%                              # Trim whitespace from the start and end of each line
  
  # Replace commas within parentheses with semicolons to avoid misinterpretation as field separators
  str_replace_all('\\(([^)]*)\\)', function(x) str_replace_all(x, ',', ';')) %>%
  
  # Format table rows by adding quotes around fields
  str_replace_all(
    '^([1-9][0-9]{0,3}),([0-9]{4,5}), ?([^,]+), ?(.*), ?(IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})$', 
    '"\\2","\\3","\\4","\\5"'
  ) %>%
  
  # Replace semicolons within parentheses back to commas
  str_replace_all('\\(([^)]*)\\)', function(x) str_replace_all(x, ';', ',')) %>%
  
  # Save the cleaned and standardized data to a new file
  write_lines(temporary_path)
```

#### Cleaning

```{r}
# Load and clean parameter data
parameter <- read_csv(temporary_path, show_col_types = FALSE) %>%
  
  # Clean all table cells: trim whitespace and replace empty strings with NA
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%   
      na_if("")        
  )) %>%  
  mutate(
    # Convert mapping to numeric, introduces NA for invalid entries  
    parameter_mapping = as.numeric(parameter_mapping),  
    # Update inconsistent parameter names
    parameter_name = case_when(
      # Experimental design procedure
      parameter_id == "IMPC_EXD_007_001" ~ "Frequency of controls",
      parameter_id == "IMPC_EXD_008_001" ~ "Number male controls",
      parameter_id == "IMPC_EXD_009_001" ~ "Number female controls",
      parameter_id == "IMPC_EXD_014_001" ~ "Core stock strategy",
      parameter_id == "IMPC_EXD_015_001" ~ "Knockout batch strategy",
      # Histopathology procedure
      parameter_id == "IMPC_HIS_008_001" ~ "Eye - Free text diagnostic term",
      parameter_id == "IMPC_HIS_010_001" ~ "Eye - Severity score",
      parameter_id == "IMPC_HIS_011_001" ~ "Eye - Descriptor PATO",
      parameter_id == "IMPC_HIS_012_001" ~ "Eye - Description",
      parameter_id == "IMPC_HIS_140_001" ~ "Testis - Free text diagnostic term",
      parameter_id == "IMPC_HIS_142_001" ~ "Testis - Severity score",
      parameter_id == "IMPC_HIS_143_001" ~ "Testis - Descriptor PATO",
      parameter_id == "IMPC_HIS_144_001" ~ "Testis - Description",
      parameter_id == "IMPC_HIS_152_001" ~ "Prostate gland - Free text diagnostic term",
      parameter_id == "IMPC_HIS_154_001" ~ "Prostate gland - Severity score",
      parameter_id == "IMPC_HIS_155_001" ~ "Prostate gland - Descriptor PATO",
      parameter_id == "IMPC_HIS_156_001" ~ "Prostate gland - Description",
      parameter_id == "IMPC_HIS_180_002" ~ "Eye - Significance score",
      parameter_id == "IMPC_HIS_202_002" ~ "Testis - Significance score",
      parameter_id == "IMPC_HIS_204_002" ~ "Prostate gland - Significance score",
      parameter_id == "IMPC_HIS_259_001" ~ "Trigeminal ganglion - Diagnostic term",
      parameter_id == "IMPC_HIS_261_001" ~ "Trigeminal ganglion - Severity score",
      parameter_id == "IMPC_HIS_262_001" ~ "Trigeminal ganglion - Descriptor PATO",
      parameter_id == "IMPC_HIS_263_001" ~ "Trigeminal ganglion - Description",
      parameter_id == "IMPC_HIS_264_001" ~ "Trigeminal ganglion - Significance score",
      # Housing and Husbandry procedure
  parameter_id == "IMPC_HOU_042_001" ~ "Nutrition - Diet Mass Known", # Nutrition - do you know the composition of the diet (average based on mass)

  parameter_id == "IMPC_HOU_046_001" ~ "Nutrition - Diet Cal Known", # Nutrition - do you know the composition of the diet (average based on calorific content)
  parameter_id == "IMPC_HOU_047_001" ~ "Nutrition - Diet Cal Carb", # Nutrition - average composition (based on calorific content) - Carbohydrate
  parameter_id == "IMPC_HOU_064_001" ~ "Microbiological Status - Pathogen Positive", # Microbiological status - is your unit positive for any of the pathogens tested
      TRUE ~ parameter_name  # Default case
    ),
    # Fill or update parameter descriptions
    parameter_description = case_when(
      parameter_id == "IMPC_EXD_015_001" ~ "Knockout batch strategy",
      parameter_id == "IMPC_HIS_008_001" ~ "Eye - Free text diagnostic term",
      parameter_id == "IMPC_HIS_010_001" ~ "Eye - Severity score",
      is.na(parameter_description) ~ tolower(str_replace_all(parameter_name, " ", "_")), # Fill missing descriptions with underscored name 
      TRUE ~ parameter_description  # Default case
    )
  ) %>%
  
  # Ensure one record per parameter
  distinct(parameter_name, parameter_description, parameter_id, .keep_all = TRUE)  # Reduces rows from 5325 to 3600; ignore parameter_mappings for this as each parameter is part of a specific procedure 

# Save the cleaned data to a new CSV file
write_csv(parameter, paste0(rootDir, "data/cleaned/cleaned_IMPC_parameter_description.csv"))
```

```{r}
# Remove unnecessary files and variables
file.remove(temporary_path)
rm(temporary_path)
```

#### Validating

```{r}
# Check the parameter table for SOP compliance
errors <- c()

# Check if parameter_mapping is numeric
if (!all(is.numeric(parameter$parameter_mapping), na.rm = TRUE)) {
  errors <- c(errors, "Some 'parameter_mapping' values are not numeric.")
}

# Check if parameter_name, parameter_description, and parameter_id are strings
if (!all(sapply(parameter$parameter_name, is.character))) {
  errors <- c(errors, "Some 'parameter_name' values are not strings.")
}
if (!all(sapply(parameter$parameter_description, is.character))) {
  errors <- c(errors, "Some 'parameter_description' values are not strings.")
}
if (!all(sapply(parameter$parameter_id, is.character))) {
  errors <- c(errors, "Some 'parameter_id' values are not strings.")
}

# Check if parameter_id are unique
if (length(unique(parameter$parameter_id)) != nrow(parameter)) {
  errors <- c(errors, "Duplicate 'parameter_id' values found.")
}

# Check if parameter_id and parameter_name length are within SOP constraints 
if (!all(nchar(parameter$parameter_id) >= 15 & nchar(parameter$parameter_id) <= 18, na.rm = TRUE)) {
  errors <- c(errors, "Some 'parameter_id' lengths are outside SOP constraints (15-18 characters).")
}
if (!all(nchar(parameter$parameter_name) >= 2 & nchar(parameter$parameter_name) <= 74, na.rm = TRUE)) {
  errors <- c(errors, "Some 'parameter_name' lengths are outside SOP constraints (2-74 characters).")
}

# Print results
if (length(errors) == 0) {
  print("All checks passed successfully.")
} else {
  print(paste("Errors found:", paste(errors, collapse = " ")))
}
```

### `disease_information.txt`

#### Formatting

```{r}
temporary_path <- paste0(rootDir, "data/cleaned/Disease_information_table.txt")
  
disease <- read_lines(paste0(rootDir, "data/metadata/Disease_information.txt")) %>% 
  .[-1] %>%    # Remove the first line (header 'x')
  str_remove_all('^\\"\\d+\\"\\s') %>% # Remove  line numbers enclosed in quotes followed by a space
  str_remove_all('^"|"$') %>%     # Remove any leading or trailing quotes
  
  # Separate fields based on the regex 
  str_replace_all(
    '(OMIM|ORPHA|DECIPHER):(\\d+),\\s*(.+?),\\s*(MGI:\\d+),\\s*(\\d+)',
    '"\\1:\\2","\\3","\\4","\\5"') %>%

  # Save the cleaned and standardized data to a new file
  write_lines(temporary_path)
```

#### Cleaning

```{r}
# Load and clean parameter data
disease <- read_csv(temporary_path, show_col_types = FALSE) %>%
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%   
      na_if("")        
  )) %>%  
  mutate(
    # Convert safely to numeric
    phenodigm_score = as.numeric(phenodigm_score)
  )%>%
  # Remove duplicates
  distinct()

# Save the cleaned data to a new CSV file
write_csv(disease, paste0(rootDir, "data/cleaned/cleaned_disease_information.csv"))

# Find rows with the same disease_id and gene_accession_id
duplicates <- disease %>%
  group_by(disease_id, gene_accession_id) %>%
  filter(n() > 1) %>%
  ungroup()

# Check if duplicates exist and display them
if (nrow(duplicates) > 0) {
  cat("Rows with the same disease_id and gene_accession_id:\n")
  print(duplicates)
} else {
  cat("No rows with duplicate disease_id and gene_accession_id found.\n")
}

```

```{r}
# Remove unnecessary files and variables
file.remove(temporary_path)
rm(temporary_path)
```

#### Validating 

```{r}
# Validate disease table for SOP compliance
errors <- c()

# Ensure disease_id, disease_term, and gene_accession_id are strings
if (!all(sapply(disease$disease_id, is.character))) {
  errors <- c(errors, "'disease_id' contains non-string values.")
}
if (!all(sapply(disease$disease_term, is.character))) {
  errors <- c(errors, "'disease_term' contains non-string values.")
}
if (!all(sapply(disease$gene_accession_id, is.character))) {
  errors <- c(errors, "'gene_accession_id' contains non-string values.")
}

# Ensure phenodigm_score is a double
if (!all(is.double(disease$phenodigm_score), na.rm = TRUE)) {
  errors <- c(errors, "'phenodigm_score' contains non-double values.")
}

# Validate disease_id format (starts with ORPHA, OMIM, or DECIPHER)
if (!all(grepl("^(ORPHA|OMIM|DECIPHER):\\d+$", disease$disease_id))) {
  errors <- c(errors, "'disease_id' does not conform to the expected format (ORPHA, OMIM, or DECIPHER followed by a colon and digits).")
}

# Validate gene_accession_id format (starts with MGI and is 9 to 11 characters)
if (!all(grepl("^MGI:\\d{5,7}$", disease$gene_accession_id))) {
  errors <- c(errors, "'gene_accession_id' does not conform to the expected format (MGI: followed by 5 to 7 digits).")
}

# Output validation results
if (length(errors) == 0) {
  print("All checks passed successfully.")
} else {
  print(paste("Errors found:", paste(errors, collapse = " ")))
}
```

# Collating

# MySQL Database

```{sql}
#| eval: false

-- Check if MySQL is running
ps -ef | grep mysql

-- Connect to MySQL
mysql -u root -p --local-infile=1
-- local-infile=1 enables data import from local files using LOAD DATA LOCAL INFILE.
```

## Create Database and Schemas

```{sql}
#| eval: false

-- Create the database
CREATE DATABASE IMPCDb;

-- Select the database
USE IMPCDb;
```

```{sql}
#| eval: false

-- Create tables without foreign keys first

-- Table: Gene
CREATE TABLE Genes (
    gene_accession_id VARCHAR(11) PRIMARY KEY,           -- SOP: 9 to 11 alphanumeric characters
    gene_symbol VARCHAR(13)                    -- SOP: 1 to 13 alphanumeric characters
) COMMENT = 'Stores gene identifiers and symbols. Referenced by Analyses and PhenodigmScores (one-to-many).';


-- Table: ProcedureTable (Procedure is a reserved keyword in SQL)
CREATE TABLE Procedures (
    procedure_id INT PRIMARY KEY,              -- Unique identifier for each procedure (impcParameterOrigId)
    procedure_name VARCHAR(47),                -- Max length: 47 characters
    procedure_description VARCHAR(1090),       -- Max length: 1090 characters
    procedure_isMandatory BOOLEAN NOT NULL     -- No missing values; TRUE/FALSE
        DEFAULT FALSE
) COMMENT = 'Stores procedure details (name, description, mandatory). Linked to Parameters via a many-to-many join.';


-- Table: Disease
CREATE TABLE Diseases (
    disease_id VARCHAR(50) PRIMARY KEY,        -- Unique identifier for each disease
    disease_term VARCHAR(150)                  -- Descriptive term for the disease
) COMMENT = 'Stores disease identifiers and terms. Linked to Genes in PhenodigmScores (many-to-many).';


-- Table: ParameterGroupings
CREATE TABLE ParameterGroupings (
    grouping_id INT PRIMARY KEY AUTO_INCREMENT,   -- Automatically increments for each new row
    grouping_name VARCHAR(50)                     -- Name of the parameter group
) COMMENT = 'Defines groups of parameters. Linked to Parameters via a many-to-many join.';


-- Table: Parameter
CREATE TABLE Parameters (
    parameter_id VARCHAR(18) PRIMARY KEY,          -- SOP: 15 to 18 characters
    parameter_name VARCHAR(74),                    -- SOP: 2 to 74 characters
    parameter_description VARCHAR(1000)            -- Descriptive field for the parameter
) COMMENT = 'Stores parameter metadata. Linked to Procedures and ParameterGroupings (many-to-many) and referenced by Analyses (one-to-many).';

-- Create tables with foreign keys

-- Table: Analysis
CREATE TABLE Analyses (
    analysis_id VARCHAR(15) PRIMARY KEY,           -- SOP: 15 alphanumeric character string
    gene_accession_id VARCHAR(11),                           -- SOP: 9 to 11 alphanumeric characters
    mouse_life_stage VARCHAR(17),                  -- SOP: 4 to 17 characters
    mouse_strain VARCHAR(5),                       -- SOP: 3 to 5 alphanumeric characters
    parameter_id VARCHAR(18),                      -- Foreign key to Parameter table
    p_value FLOAT,                                 -- SOP: Float from 0 to 1
    FOREIGN KEY (gene_accession_id) REFERENCES Genes(gene_accession_id),
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id)
) COMMENT = 'Stores analysis results linking Genes and Parameters (many-to-one), including p-values and strain info.';


-- Table: GeneDisease
CREATE TABLE PhenodigmScores (
    phenodigm_id INT AUTO_INCREMENT PRIMARY KEY,

    disease_id VARCHAR(50),                        -- Foreign key to Disease table
    gene_accession_id VARCHAR(11),                           -- Foreign key to Gene table
    phenodigm_score FLOAT,                         -- Phenodigm association score
    FOREIGN KEY (disease_id) REFERENCES Diseases(disease_id),
    FOREIGN KEY (gene_accession_id) REFERENCES Genes(gene_accession_id)
) COMMENT = 'Stores gene-disease association scores (many-to-many) linking Genes and Diseases.';


-- Create join tables

-- Table: ParameterProcedure
CREATE TABLE parameterXprocedure (
    parameter_id VARCHAR(18),                      -- Foreign key to Parameter table
    procedure_id INT,                              -- Foreign key to ProcedureTable
    PRIMARY KEY (parameter_id, procedure_id),      -- Composite primary key to ensure uniqueness
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id),
    FOREIGN KEY (procedure_id) REFERENCES Procedures(procedure_id)
) COMMENT = 'Join table linking Parameters to Procedures (many-to-many).';


-- Table: ParameterGroup
CREATE TABLE parameterXgroup (
    grouping_id INT,                               -- Foreign key to ParameterGroupings table
    parameter_id VARCHAR(18),                      -- Foreign key to Parameter table
    PRIMARY KEY (grouping_id, parameter_id),       -- Composite primary key to ensure uniqueness
    FOREIGN KEY (grouping_id) REFERENCES ParameterGroupings(grouping_id),
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id)
) COMMENT = 'Join table linking Parameters to ParameterGroupings (many-to-many).';
```

## Populate Database

To populate the database, the schema should first be examined to identify which attributes correspond to each table. Data should then be organized into separate CSV or TSV files, with each file representing a single table. Each file is expected to include a header row (optional but recommended) that matches the column names defined in the database schema.

Once a CSV file has been created for each table, the data can be loaded directly into MySQL using the `LOAD DATA INFILE` command.

-   **`FIELDS TERMINATED BY ','`**: Indicates that fields are separated by commas.

-   **`OPTIONALLY ENCLOSED BY '"'`**: Handles strings enclosed in double quotes.

-   **`IGNORE 1 LINES`**: Skips the header row in the CSV, if present.

### Procedure Table

```{r}
procedures <- procedure %>%
  select(procedure_id, procedure_name, procedure_description, is_mandatory) %>%  # Select specific columns
  distinct() # Keep only unique rows (this shrinks the table from 5311 rows to 50 rows)

# Save the table as CSV 
write_csv(procedures, paste0(rootDir, "/data/collated/Procedures.csv"))
rm(procedures)
```

```{sql}
#| eval: false
-- Path to tables 
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/cleaned/Procedures.csv'
INTO TABLE Procedures
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

### Genes Table

```{r}
# Load analysis table 
analysis <- read_csv(cleaned_procedure_path)
disease <- read_delim(paste0(rootDir, "data/cleaned/cleaned_Disease_information.txt"), delim = "\t")
head(disease)

# Subset the table, ensure unique rows, convert gene symbols to title case, and rename
genes <- analysis %>%
  clean_names() %>% 
  mutate(gene_symbol = str_to_title(gene_symbol)) %>%  # Convert gene_symbol to title case 
  select(gene_accession_id, gene_symbol) %>%          # Select specific columns
  distinct() %>%                                      # Keep only unique rows
  rename(gene_accession_id = gene_accession_id)                 # Rename columns

# Find `gene_accession_id`s in `disease` table that are not in `genes`
missing_genes <- disease %>%
  select(gene_accession_id) %>%                # Extract gene IDs from `disease`
  rename(gene_accession_id = gene_accession_id) %>%
  distinct() %>%                               # Ensure unique rows
  filter(!gene_accession_id %in% genes$gene_accession_id)          # Find those not in `genes`

# Add missing `gene_accession_id`s to `genes` with `gene_symbol` as NULL; here we could manually search them and add them! 
genes <- genes %>%
  bind_rows(missing_genes %>% 
  mutate(gene_symbol = NA)) %>% # Add missing genes with NA for gene_symbol
  distinct()                    # Ensure no duplicates

head(genes)

# Save the table as CSV 
write.csv(genes, file = paste0(rootDir, "/data/collated/Genes.csv"), row.names = FALSE, quote = TRUE)
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Genes.csv'
INTO TABLE Genes
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

### Parameters Table

```{r}
# Load analysis table 
parameter <- read_delim(paste0(rootDir, "data/cleaned/cleaned_IMPC_parameter_description.txt"), delim = ",")


# Select the subset of columns required for the Parameters table
parameters <- parameter %>%
  clean_names() %>% 
  mutate_all(~ str_trim(.)) %>%                       # Trim leading/trailing whitespace
  mutate_all(~ ifelse(. %in% c("", "NA"), NA, .)) %>% # Replace empty strings or "NA" with actual NA
  select(parameter_id, name, description) %>%          # Select only the specific columns needed
  rename(parameter_id = parameter_id,                  # Rename the columns
         parameter_name = name,                      
         parameter_description = description) %>%
  group_by(parameter_id) %>%                          # Group by parameter_id
  slice_min(nchar(parameter_name), with_ties = FALSE) %>% # Keep the row with the shortest parameter_name
  ungroup()                                     # Remove grouping

parameters[(nchar(parameters$parameter_name))> 74, ]
# Save the table as CSV 
write.csv(parameters, file = paste0(rootDir, "/data/collated/Parameters.csv"), row.names = FALSE, quote = TRUE)
```

*\*Some of the IDs appeared to be duplicated, but upon inspection, the rows contained equivalent values (e.g., for IMPC_HIS_010_001: "Eye with optic nerve - Severity score" and "Eye - Severity score"). Given the same identifier and the similarity of these terms, it is reasonable to assume that they refer to the same parameter, and only one of the duplicates was uploaded to the database.*

![](images/Screenshot%202024-12-18%20at%2015.27.00.png)

*Some values are truncated, which aligns with the SOP specifying a maximum of 74 characters. However, the maximum number of characters in the dataset is 88, exceeding the allowed limit.*

![](images/Screenshot%202024-12-18%20at%2015.26.12.png)

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Parameters.csv'
INTO TABLE Parameters
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

### Disease Table

```{r}
# Select the subset of columns required for the Parameters table
diseases <- disease %>%
  clean_names() %>% 
  select(disease_id, disease_term) %>%          # Select only the specific columns needed
  distinct()

write_csv(diseases, paste0(rootDir, "/data/collated/Diseases.csv")) 
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Diseases.csv'
INTO TABLE Diseases
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

### PhenoDigm Score Table

```{r}
# Select the subset of columns required for the Parameters table
phenodigm_scores <- disease %>%
  clean_names() %>% 
  select(disease_id, gene_accession_id, phenodigm_score) %>%          # Select only the specific columns needed
  distinct()

write_csv(phenodigm_scores, paste0(rootDir, "/data/collated/PhenodigmScores.csv")) 
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/PhenodigmScores.csv'
INTO TABLE PhenodigmScores
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
LINES TERMINATED BY '\n' 
IGNORE 1 LINES
(disease_id, gene_accession_id, phenodigm_score); -- so that the phenodigm_id is created dynamically
```

## parameterXprocedure

```{r}
# Join the parameters table and the procedure table using "parameter_mapping" and "procedure_id"
parameterXprocedure <- procedure %>%
  left_join(parameter, by = c("parameter_mapping" = "procedure_id")) %>%
  select(procedure_id, parameter_id) %>%  # Retain only the relevant columns
  distinct() # Ensure no duplicates (this shrinks the table from 5311 rows to 3600 rows)

# Save the resulting table as a CSV file
write_csv(parameterXprocedure, paste0(rootDir, "/data/collated/parameterXprocedure.csv"))
# Remove unused files 
rm(parameterXprocedure)
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/parameterXprocedure.csv'
INTO TABLE parameterXprocedure
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 LINES; -- so that the phenodigm_id is created dynamically
```

### Analysis Table

```{r}
# Load analysis table 
analysis <- read_delim(paste0(rootDir, "data/cleaned/analysis_table.txt"), delim = ",")

head(analysis)
analyses <- analysis %>%
  clean_names() %>% 
  mutate_all(~ str_trim(.)) %>%                       # Trim leading/trailing whitespace
  mutate_all(~ ifelse(. %in% c("", "NA"), NA, .)) %>% # Replace empty strings or "NA" with actual NA
  select(analysis_id, gene_accession_id, mouse_strain, mouse_life_stage, parameter_id, pvalue)   %>%  
    rename(# Select only the specific columns needed  rename(parameter_id = parameter_id,                  # Rename the columns
      p_value = pvalue)   %>%  
    distinct()
head(analyses)
write_csv(analyses, paste0(rootDir, "/data/collated/Analyses.csv"))
rm(analyses)
```

```{sql}
#| eval: false
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Analyses.csv'
INTO TABLE Analyses
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
LINES TERMINATED BY '\n'
IGNORE 1 LINES; -- so that the phenodigm_id is created dynamically
```

## Determining Groupings

To reduce the parameter space the  collaborator would like to start grouping parameters together based on phenotype test similarity and/or naming similarity. Design the database to reflect new groupings for weight,  images and brain parameters, plus at least 3 others of your choice (refer to below). 

In addition to the data provided, each group must  find at least 3 other parameter groupings from the parameter list and add this to the  database. We encourage you to find as many additional parameter groups as you can and  time permits, however only 3 more is compulsory. parameters

```{r}
parameters <- parameters %>%
  mutate(
    parameter_group = case_when(
      str_detect(parameter_name, regex(
        "weight|density|fat|lean|BMI|BMC|mass|composition|bone|length|adipose|lipid|body size|size|growth|skeletal|area|volume", 
        ignore_case = TRUE)) ~ "Weight",
      str_detect(parameter_name, regex(
        "image|screenshot|microscope|pdf|lacz|waveform|ogram|resolution|visual|pixel|scan|photo|imaging|bit depth|microscopy|contrast", 
        ignore_case = TRUE)) ~ "Images",
      str_detect(parameter_name, regex(
        "brain|cortex|hippo|thalamus|amygdala|lobe|cere|spinal|pituitary|striatum|hypothal|neural|neuronal|pons|medulla|ganglion|nerve|enceph|ventricle|choroid", 
        ignore_case = TRUE)) ~ "Brain",
      str_detect(parameter_name, regex(
        "heart|cardiac|vascular|blood|vein|aort|arter|HR|stroke|ejection|QT|circulation|pulse|pressure|rate|systolic|diastolic|respiration|ECG|cardio|valve", 
        ignore_case = TRUE)) ~ "Cardiovascular"
      )
)

parameters %>%
  group_by(parameter_group) %>%
  summarise(total_parameters = n()) %>%
  arrange(desc(total_parameters))

parameters$parameter_name[is.na(parameters$parameter_group)]

```

## Create dump

```{bash}
/opt/homebrew/bin/mysqldump IMPCDb -u root -p > /Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/database/IMPCDb.dump
```

# R/Shiny

**Test R/Shiny 1:** They want the ability to select a particular knockout mouse and visualise the statistical scores of all phenotypes tested.

```{sql}
#| eval: false
SELECT Analyses.p_value, Parameters.parameter_name 
FROM Analyses 
JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id 
WHERE Analyses.gene_accession_id IN (
    SELECT gene_accession_id 
    FROM Genes 
    WHERE gene_symbol IN ('Ifi204', 'Mettl21c', 'Ier5', 'Abca4')
)
ORDER BY Analyses.p_value ASC;
```

![](images/Screenshot%202024-12-21%20at%2016.21.52.png)

**Test R/Shiny 2:** The collaborator also wishes to visualise the statistical scores of all knockout mice for a selected phenotype.

```{sql}
#| eval: false
SELECT Analyses.p_value, Genes.gene_symbol 
FROM Analyses 
JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id 
JOIN Genes ON Analyses.gene_accession_id = Genes.gene_accession_id 
WHERE Parameters.parameter_name IN (
    'Facial Cleft',
    'Lens Opacity',
    'Triglycerides',
    'Skin color - tail',
    'Potassium',
    'HRV',
    'Hindbrain morphology',
    'Aspartate aminotransferase',
    'Tibia length',
    'Left total retinal thickness',
    'Embryo Size',
    'Effector CD4+ T helper cells - % of live leukocytes (Panel A)',
    'Limb morphology',
    'Coat - color - back',
    'Aggression',
    'Joints'
)
ORDER BY Analyses.p_value ASC;
```

![](images/Screenshot%202024-12-21%20at%2016.22.09.png)

**Test R/Shiny 3:** The collaborator would like to visualise clusters of genes with similar phenotype scores.  

```{sql}
#| eval: false

SELECT Genes.gene_symbol, Parameters.parameter_name, Analyses.p_value FROM Analyses JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id JOIN Genes ON Analyses.gene_accession_id = Genes.gene_accession_id; 
```

![](images/Screenshot%202024-12-21%20at%2016.22.33.png)
