---
title: "DCDM Project Log"
author: "Alicia"
editor: visual
format: html
---

# TRE Egress

Ella - is there anything to write here?

# Data Cleaning

```{r}
rm(list = ls())

# Install and load Tidyverse packages
# install.packages("tidyverse")
library(tidyverse)
```

```{r}
# Define the working directory path in r
rootDir = "/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
```

## Cleaning the Raw Data Files

### Quality Control/Cleaning Using the SOP (Esta)

The experiment data comes with an SOP specifying constraints for each field, including data type, minimum and maximum values for numerical data, length for strings, and specific values for certain fields. Data quality can be assessed by comparing the raw data to these predefined constraints.

Here, we have cleaned all the csv files by modifying the row names to match those specified in the SOP file.

First, we define the directory paths & the log file path.

```{r}
# Defing directory paths
data_dir <- "raw_data"
output_dir <- "updated_data"
# New log file
log_file <- "row_name_modification.log"

# Creates output directory if it doesn't already exist & supresses warnings if it does exist
dir.create(output_dir, showWarnings = FALSE)

# Clear the log file if it exists or creates it if it doesn't
file.create(log_file)
```

Then we load the SOP file. Here, I have normalized the row names to lowercase although this isn't strictly necessary for this specific file.

```{r}
# Load the SOP file and normalize row names to lowercase
expected_row_names <- tolower(read_csv("metadata/IMPC_SOP.csv", show_col_types = FALSE)$dataField)

```

Now that we have the correct files loaded and the directories specified, we define a function to do three things: 1). Identify spelling mistakes in the existing row names and fix them (in these files, only uppercase to lowercase transformation is needed). 2). Add any missing rows and assign an NA value to the second column for the corresponding row(s) - this is only done after modifying misspelt row names. 3). Reordering the rows to match the SOP layout.

```{r}

# Function to validate, update, and reorder row names
process_file <- function(file_path) {
  # Read the file specified by file_path
  data <- tryCatch(
    read_csv(file_path, col_names = TRUE, show_col_types = FALSE),
    error = function(e) {
      message(paste("Error reading file:", file_path, "-", e))
      return(NULL)
    }
  )
  if (is.null(data)) return(FALSE)
  
  # Normalize row names and identify missing rows
  data[[1]] <- tolower(data[[1]])
  missing_rows <- setdiff(expected_row_names, data[[1]])
  
  # Add missing rows with NA values in the second column
  if (length(missing_rows) > 0) {
    missing_data <- tibble(
      !!colnames(data)[1] := missing_rows,  # First column: missing row names
      !!colnames(data)[2] := NA            # Second column: NA values
    )
    data <- bind_rows(data, missing_data)
  }
  
  # Reorder rows and save the updated file
  data <- data %>% arrange(match(data[[1]], expected_row_names))
  write_csv(data, file.path(output_dir, basename(file_path)))
  
  return(TRUE)
}
```

We now use this function on all the data files - this will produce an "updated_data" folder and a log file in the main directory.

```{r}
# Process all files and log results
files <- list.files(data_dir, pattern = "\\.csv$", full.names = TRUE)
validation_results <- sapply(files, process_file)

# Write summary to log
summary_message <- sprintf(
  "Validation Summary:\nTotal files checked: %d\nTotal updated files: %d\n",
  length(validation_results), sum(validation_results)
)
cat(summary_message, file = log_file)
message(summary_message)
```

The summary message displayed the number of files validated out of the total number in the raw_data folder - this will be be displayed on the console and in the log file. The log file has been reduced down to just contain that information but was originally carried out on a subset of 600 files and clearly displayed the row name differences between those files and the SOP.

Once the file modification is done, we run one final validation only script to make sure there are no unexpected row name differences.

```{r}
data_dir <- "updated_data"  # Directory containing the updated data files
log_file <- "row_name_validation.log"  # Log file for results

# Load SOP and normalize row names
expected_row_names <- tolower(read_csv("metadata/IMPC_SOP.csv", show_col_types = FALSE)$dataField)

# Clear the log file
file.create(log_file)

# Function to validate a file against the SOP layout and column count
validate_file <- function(file_path, expected_row_names) {
  # Read the file
  data <- tryCatch(
    read_csv(file_path, col_names = TRUE, show_col_types = FALSE),
    error = function(e) {
      message(paste("Error reading file:", file_path, "-", e))
      return(list(valid = FALSE, message = "Error reading file"))
    }
  )
  
  # Skip files that could not be read
  if (is.null(data)) return(list(valid = FALSE, message = "File could not be read"))
  
  # Check column count
  if (ncol(data) != 2) {
    return(list(valid = FALSE, message = paste("Invalid column count:", ncol(data), "(expected: 2)")))
  }
  
  # Normalize row names (first column) and check against SOP
  row_names <- tolower(data[[1]])
  missing_rows <- setdiff(expected_row_names, row_names)
  extra_rows <- setdiff(row_names, expected_row_names)
  
  # Validate row names
  if (length(missing_rows) > 0 || length(extra_rows) > 0) {
    message <- ""
    if (length(missing_rows) > 0) {
      message <- paste(message, "Missing row names:", paste(missing_rows, collapse = ", "), "\n")
    }
    if (length(extra_rows) > 0) {
      message <- paste(message, "Extra row names:", paste(extra_rows, collapse = ", "), "\n")
    }
    return(list(valid = FALSE, message = message))
  }
  
  return(list(valid = TRUE, message = "File is valid"))
}

# Process all files in the data directory
files <- list.files(data_dir, pattern = "\\.csv$", full.names = TRUE)
validation_results <- lapply(files, function(file_path) {
  result <- validate_file(file_path, expected_row_names)
  message <- paste(basename(file_path), ":", result$message, "\n")
  cat(message, file = log_file, append = TRUE)
  return(result$valid)
})

# Summarize results
valid_files <- sum(unlist(validation_results))
total_files <- length(validation_results)

# Write summary to log
summary_message <- paste(
  "Validation Summary:\n",
  "Total files checked:", total_files, "\n",
  "Total valid files:", valid_files, "\n"
)
cat(summary_message, file = log_file, append = TRUE)
message(summary_message)
```

These scripts can be run locally & on HPC with a few minimal changes (i.e. loading individual dplyr & readr packages rather than whole tidyverse package).

### Combining the Raw Data in a Table (Ella?)

--\> This step can be done in bash after quality control (collating by Ella).

The dataset comprises thousands of CSV files. The key fields include: analysis_id, pvalue, parameter_name, gene_symbol, mouse_strain, parameter_id, gene_accession_id, mouse_life_stage.

To ensure efficient quality checks and facilitate further analysis, it is essential to combine all these files into a single, unified table based on their common fields.

Here’s a function that reads data from a file and transforms it into a row for a larger table:

```{r}
# Function to read a file and transform its contents into a single row
read_and_transform <- function(file) {
  
  # Read the file
  data <- read.csv(file, header = FALSE, col.names = c("field", "value"))
  # - header = FALSE: Tells R to treat the first row as regular data, not column names.
  # - col.names = c("field", "value"): Assigns consistent column names to facilitate clear and reliable subsetting in subsequent processing.

  # Convert all field names to lowercase for uniformity
  data$field <- tolower(data$field)
  
  # Transpose the 'value' column so that fields become column names
  transformed_data <- t(data$value) %>% 
    as.data.frame() %>% 
    setNames(data$field)
  
  # Rearrange columns to place 'analysis_id' as the first column
  transformed_data <- transformed_data %>%
    relocate(analysis_id, .before = everything())

  # Return the transformed data as a single-row data frame
  return(transformed_data)
}
```

The function can now be applied to all CSV files in the `raw_data` directory to combine them into a single table using the `map_dfr()` function from the **`purrr`** package.

```{r}
# List all CSV files in the raw_data directory
all_files <- list.files(paste0(rootDir, "data/raw_data"), pattern = "\\.csv$", full.names = TRUE)

# Apply the read_and_transform function to each file and combine results into a single data frame
data_table <- purrr::map_dfr(all_files, read_and_transform)
# Display the first 6 rows of the data table
head(data_table)

# Save the combined table to a new CSV file
write.csv(data_table, paste0(rootDir, "/temporary/data_table.csv"), row.names = FALSE)
```

2nd Column Data Cleaning... (To be done after collating data on HPC??? - Esta)

For each column, we need to check similar things, like whether the data is the correct type (e.g., string or number), whether it has the right length (for strings) or size (for floats), and whether it matches allowed values, like for `mouse_strain` and `mouse_life_stage`. We may also need to find missing values. For each issue, we want to print the value and its index. To make this process less repetitive, we can create reusable functions.

Cleaning the Metadata Files

### `IMPC_procedure.txt (Alicia)`

Each row includes: 1) a line number 2) a name 3) a description 4) a boolean indicating if the procedure is mandatory 5) the original IMPC ID.

**Issues**:

-   Unnecessary double quotes surrounding columns –\> Remove unnecessary quotes and fix column separators.

-   Commas in the description column misinterpreted as separators –\>Enclose the description column in double quotes.

-   `line_number` and `procedureId` duplicate the same unique information –\> Replace `line_number` with `procedureId`.

-   HTML symbols appear incorrectly encoded but will be ignored for simplicity –\> Clean file encoding and ignore invalid characters.

```{bash}
# Define working directory path in bash
rootDir="/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"

# Clean the IMPC_procedure.txt file
sed 's/^"//;s/"$//;s/" "/,/g' "${rootDir}/data/metadata/IMPC_procedure.txt" | 
sed -E 's/^([0-9]+,[^,]+,)(.*)(, (TRUE|FALSE), [0-9]{5})$/\1"\2"\3/' |
sed -E 's/^line_number,//' |
iconv -f UTF-8 -t UTF-8//IGNORE > "${rootDir}/data/metadata/cleaned_IMPC_procedure.txt"

# Completion message
echo "IMPC_procedure.txt successfully cleaned and saved."

# perl -MHTML::Entities -pe 'decode_entities($_);'> "${rootDir}/data/metadata/cleaned_IMPC_procedure.txt"
```

```{r}
# Load the cleaned procedure data, setting procedureId as row names
procedure <- read.table(
  file = paste0(rootDir, "data/metadata/cleaned_IMPC_procedure.txt"),
  header = TRUE,                
  sep = ",",                    
  quote = '"',                  
)

# Display the first few rows of the data
head(procedure)

# Find the maximum string length in the 'name' column
max(nchar(procedure$name))
# Find the maximum string length in the 'description' column
max(nchar(procedure$description))
# Count the number of duplicated values in the 'impcParameterOrigId' column
sum(duplicated(procedure$impcParameterOrigId))
# Count the number of missing (NA) values in the 'isMandatory' column
sum(is.na(procedure$isMandatory))
```

Since the file loads correctly in R, we can confirm that the regular expression (REGEX) accurately matched the columns when enclosing the description. This ensures that each column is in the correct format, so there is no need to further check the column formatting. Additionally, the **procedureId** and **impcParameterOrigId** columns don't contain duplicated values.

### `IMPC_parameter_description.txt (Sama)`

```{bash}
#| eval: false

# Define working directory path in bash
rootDir="/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
sed -E 's/^"//; s/"$//; s/" "/,/g;' "${rootDir}/data/metadata/IMPC_parameter_description.txt" |
sed -E 's/^([0-9]{1,4},[0-9]{5}, )(.*)(,[^,]+, IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})$/\1"\2"\3/' |
sed -E 's/^x//' > "${rootDir}/data/metadata/cleaned_IMPC_parameter_description.txt"

```

```{r}
#| eval: false

# Load the cleaned parameter data
parameter_description <- read.table(
  file = paste0(rootDir, "data/metadata/cleaned_IMPC_parameter_description.txt"), # File path
  header = TRUE,                 # First row contains column names
  sep = ",",                     # Fields are separated by commas
  stringsAsFactors = FALSE,      # Prevent character columns from converting to factors
  check.names = FALSE            # Keep column names as-is, even if they contain spaces or special characters
)

# Display the loaded data in a tabular format
head(parameter_description)
```

### `disease_information.txt (Sanj)`

The Disease_information.txt file contains metadata with four key columns: disease_id, disease_term, gene_accession_id, and phenodigm_score.

Issues:

-   Unecessary rows and formatting (redundant "x" row at the top, row numbers and surrounding double quotes embedded within data) –\> Remove unnecessary rows, row numbers and extraneous quotes

-   The disease_term column contains commas misinterpreted as separators -\> Combine split fields to ensure disease_term remains intact

-   phenodigm_score contains non-numeric characters and extra spaces -\> Clean non-numeric values and safely convert to numeric format

-   Row numbers and quotes appear in the disease_id column -\> Clean disease_id to remove row numbers and quotes

-   Duplicate disease_id values exist –\> We need to decide if they are meaningful (they have unique phenodigm scores for reference)

Let's review what the data looks like.

```{r}

install.packages(c("tidyverse", "janitor", "stringr"))
library(tidyverse)
library(janitor)
library(stringr)

# Load raw file
raw_lines <- readLines("/Users/sanjanasrinivasan/Desktop/DCDM_IMPC_Project/data/metadata/Disease_information.txt")

# Inspect the first few rows
head(raw_lines, 13)
```

Key Observations: 1)An unnecessary x row exists at the top. 2) Row numbers and extra quotes are embedded in the data. disease_term fields contain commas causing extra splits.

To prepare the data for further processing:1) Remove the first "x" row. 2) Strip row numbers and surrounding quotes.

```{r}
# Remove the "x" row
cleaned_lines <- raw_lines[-1]  # Remove the first row ("x")

# Remove row numbers and surrounding quotes
cleaned_lines <- cleaned_lines %>%
  str_remove_all('^\\d+\\"\\s') %>%  # Remove row numbers (e.g., "1") and following quote-space
  str_remove_all('^"|"$')           # Remove the outer quotes


# Inspect the cleaned lines
head(cleaned_lines, 14)
```

Some disease_term fields contain commas, splitting the data into extra columns. Here, we fix that issue by combining split fields.

```{r}
# Function to process and clean split rows
process_split_row <- function(row) {
  # Split the row by commas outside quotes
  split_row <- str_split(row, ",\\s*(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)")[[1]]
  
  # Check if the split_row has more than 4 fields (indicating a split disease term)
  if (length(split_row) > 4) {
    # Combine extra fields in the disease_term column
    disease_term <- paste(split_row[2:(length(split_row) - 2)], collapse = ", ")
    return(c(split_row[1], disease_term, split_row[length(split_row) - 1], split_row[length(split_row)]))
  } else {
    # Return the row as is if it's already properly split
    return(split_row)
  }
}

# Apply the function to all cleaned lines
split_data <- map(cleaned_lines, process_split_row)

#Inspect split_data
str(split_data) 

# Inspect a sample of split_data, which should now all have exactly 4 columns
head(split_data[20])

# Checking for any remaining problematic rows (e.g., empty or improperly split rows)
problematic_rows <- which(map_int(split_data, length) != 4)
print("Indices of problematic rows:")
print(problematic_rows)     #All rows are well split into 4 columns now

# Create data_frame from split_data 
data_frame <- split_data %>%
  map(~ set_names(as.list(.), c("disease_id", "disease_term", "gene_accession_id", "phenodigm_score"))) %>%
  bind_rows()

```

The phenodigm_score column contains non-numeric characters and spaces. We clean and convert it safely to numeric.

```{r}

# Clean the phenodigm_score column
data_frame <- data_frame %>%
  mutate(
    phenodigm_score = str_trim(phenodigm_score),  # Remove leading/trailing spaces
    phenodigm_score = str_replace_all(phenodigm_score, "[^0-9\\.]", "")  # Remove non-numeric characters except "."
  )

#After cleaning, convert the column to numeric.
data_frame <- data_frame %>%
  mutate(
    phenodigm_score = as.numeric(phenodigm_score)
  )

#After cleaning, check for any remaining NAs
remaining_na <- data_frame %>%
  filter(is.na(phenodigm_score))

print("Rows with remaining NA values in phenodigm_score:")
print(remaining_na)

#Inspect the data to verify changes
str(data_frame)
summary(data_frame)
```

Final Cleanup: Fix Rows and Disease IDs 1) Drop the extra header row. 2) Clean disease_id to remove row numbers and quotes.

```{r}
# Remove the first row
data_frame <- data_frame[-1, ]  # Drop the first row
head(data_frame)  #Checking for change

# Clean disease_id column
data_frame <- data_frame %>%
  mutate(
    disease_id = str_remove(disease_id, '^\\d+\\"\\s'), # Remove row numbers and quotes
    disease_id = str_remove_all(disease_id, '^"|"$')    # Remove any remaining outer quotes
  )

#Verify changes
head(data_frame)

```

Final Quality Checks: Verify the dataset for: correct data types, missing values, duplicates, proper formatting

```{r}
# Verify data types
str(data_frame)

# Confirm numeric values for phenodigm_score
summary(data_frame$phenodigm_score)

# Count missing values in each column
colSums(is.na(data_frame)) #shows there are no missing values in any of the columns

# Verify uniqueness of disease_id
unique_disease_ids <- nrow(data_frame) == length(unique(data_frame$disease_id))

if (unique_disease_ids) {
  print("All disease_id values are unique!")
} else {
  print("Warning: Duplicate disease_id values found.")
} #Duplicate disease_id values found!!!

# Check disease_id format (e.g., OMIM:xxxx or ORPHA:xxxx)
invalid_disease_ids <- data_frame %>%
  filter(!str_detect(disease_id, "^(OMIM|ORPHA):\\d+$"))

print("Invalid disease_id values:")
print(invalid_disease_ids) #none

# Check gene_accession_id format (e.g., MGI:xxxxx)
invalid_gene_ids <- data_frame %>%
  filter(!str_detect(gene_accession_id, "^MGI:\\d+$"))

print("Invalid gene_accession_id values:")
print(invalid_gene_ids) #none

```

Here, we leave the duplicate disease_ids because our database schema will reflect these many to many relationships accordingly.

Saving and Loading the Final Data

```{r}
# Save data_frame as a tab-delimited text file
write.table(data_frame, file = "disease_information.txt", sep = "\t", row.names = FALSE, quote = FALSE)

# Load the final cleaned data
disease_information <- read.table(
  "disease_information.txt", 
  header = TRUE,          # Use the first row as column names 
  sep = "\t",             # Tab-delimited
  stringsAsFactors = FALSE) # Avoid character columns from converting to factors

# Verify the loaded data
str(disease_information)

#Display the cleaned and loaded data in tabular format
head(disease_information)

```

# Collating (Ella)

# MySQL Database

```{sql}
#| eval: false

-- Check if MySQL is running
ps -ef | grep mysql

-- Connect to MySQL (use the password created at the start of the course)
mysql -u root -p --local-infile=1

```

## Create Database and Schemas

```{sql}
#| eval: false

-- Create the database in MySQL 
CREATE DATABASE IMPCDb;
-- Use the newly created database in subsequent operatoins
USE IMPCDb;
```

```{sql}
#| eval: false
-- Create tables without foreign keys first

-- Table: Gene
CREATE TABLE Genes (
    gene_id VARCHAR(11) PRIMARY KEY,           -- SOP: 9 to 11 alphanumeric characters
    gene_symbol VARCHAR(13)                    -- SOP: 1 to 13 alphanumeric characters
) COMMENT = 'Stores gene identifiers and symbols. Referenced by Analyses and PhenodigmScores (one-to-many).';


-- Table: ProcedureTable (Procedure is a reserved keyword in SQL)
CREATE TABLE Procedures (
    procedure_id INT PRIMARY KEY,              -- Unique identifier for each procedure (impcParameterOrigId)
    procedure_name VARCHAR(47),                -- Max length: 47 characters
    procedure_description VARCHAR(1090),       -- Max length: 1090 characters
    procedure_isMandatory BOOLEAN NOT NULL     -- No missing values; TRUE/FALSE
        DEFAULT FALSE
) COMMENT = 'Stores procedure details (name, description, mandatory). Linked to Parameters via a many-to-many join.';


-- Table: Disease
CREATE TABLE Diseases (
    disease_id VARCHAR(50) PRIMARY KEY,        -- Unique identifier for each disease
    disease_term VARCHAR(150)                  -- Descriptive term for the disease
) COMMENT = 'Stores disease identifiers and terms. Linked to Genes in PhenodigmScores (many-to-many).';


-- Table: ParameterGroupings
CREATE TABLE ParameterGroupings (
    grouping_id INT PRIMARY KEY AUTO_INCREMENT,   -- Automatically increments for each new row
    grouping_name VARCHAR(50)                     -- Name of the parameter group
) COMMENT = 'Defines groups of parameters. Linked to Parameters via a many-to-many join.';


-- Table: Parameter
CREATE TABLE Parameters (
    parameter_id VARCHAR(18) PRIMARY KEY,          -- SOP: 15 to 18 characters
    parameter_name VARCHAR(74),                    -- SOP: 2 to 74 characters
    parameter_description VARCHAR(1000)            -- Descriptive field for the parameter
) COMMENT = 'Stores parameter metadata. Linked to Procedures and ParameterGroupings (many-to-many) and referenced by Analyses (one-to-many).';

-- Create tables with foreign keys

-- Table: Analysis
CREATE TABLE Analyses (
    analysis_id VARCHAR(15) PRIMARY KEY,           -- SOP: 15 alphanumeric character string
    gene_id VARCHAR(11),                           -- SOP: 9 to 11 alphanumeric characters
    mouse_life_stage VARCHAR(17),                  -- SOP: 4 to 17 characters
    mouse_strain VARCHAR(5),                       -- SOP: 3 to 5 alphanumeric characters
    parameter_id VARCHAR(18),                      -- Foreign key to Parameter table
    p_value FLOAT,                                 -- SOP: Float from 0 to 1
    FOREIGN KEY (gene_id) REFERENCES Genes(gene_id),
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id)
) COMMENT = 'Stores analysis results linking Genes and Parameters (many-to-one), including p-values and strain info.';


-- Table: GeneDisease
CREATE TABLE PhenodigmScores (
    phenodigm_id INT AUTO_INCREMENT PRIMARY KEY,

    disease_id VARCHAR(50),                        -- Foreign key to Disease table
    gene_id VARCHAR(11),                           -- Foreign key to Gene table
    phenodigm_score FLOAT,                         -- Phenodigm association score
    FOREIGN KEY (disease_id) REFERENCES Diseases(disease_id),
    FOREIGN KEY (gene_id) REFERENCES Genes(gene_id)
) COMMENT = 'Stores gene-disease association scores (many-to-many) linking Genes and Diseases.';


-- Create join tables

-- Table: ParameterProcedure
CREATE TABLE parameterXprocedure (
    parameter_id VARCHAR(18),                      -- Foreign key to Parameter table
    procedure_id INT,                              -- Foreign key to ProcedureTable
    PRIMARY KEY (parameter_id, procedure_id),      -- Composite primary key to ensure uniqueness
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id),
    FOREIGN KEY (procedure_id) REFERENCES Procedures(procedure_id)
) COMMENT = 'Join table linking Parameters to Procedures (many-to-many).';


-- Table: ParameterGroup
CREATE TABLE parameterXgroup (
    grouping_id INT,                               -- Foreign key to ParameterGroupings table
    parameter_id VARCHAR(18),                      -- Foreign key to Parameter table
    PRIMARY KEY (grouping_id, parameter_id),       -- Composite primary key to ensure uniqueness
    FOREIGN KEY (grouping_id) REFERENCES ParameterGroupings(grouping_id),
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id)
) COMMENT = 'Join table linking Parameters to ParameterGroupings (many-to-many).';
```

```{sql}
#| eval: false
-- To explore the database
SHOW TABLES;
DESCRIBE Gene;
DESCRIBE ProcedureTable;
DESCRIBE Analysis;
DESCRIBE Parameter;
DESCRIBE ParameterGrouping;
DESCRIBE Disease;
DESCRIBE GeneDisease;
```

## Populate Database

To populate the database, the schema should first be examined to identify which attributes correspond to each table. Data should then be organized into separate CSV or TSV files, with each file representing a single table. Each file is expected to include a header row (optional but recommended) that matches the column names defined in the database schema.

Once a CSV file has been created for each table, the data can be loaded directly into MySQL using the `LOAD DATA INFILE` command.

-   **`FIELDS TERMINATED BY ','`**: Indicates that fields are separated by commas.

-   **`OPTIONALLY ENCLOSED BY '"'`**: Handles strings enclosed in double quotes.

-   **`IGNORE 1 LINES`**: Skips the header row in the CSV, if present.

### Procedure Table

```{r}
# Subset, rename columns, and select unique rows
procedures <- procedure %>%
  select(impcParameterOrigId, name, description, isMandatory) %>%  # Select specific columns
  distinct() %>%                                                   # Keep only unique rows
  rename(procedure_id = impcParameterOrigId,                       # Rename columns
         procedure_name = name,
         procedure_description = description,
         procedure_isMandatory = isMandatory)

# Save the table as CSV 
write.csv(procedures, file = paste0(rootDir, "/data/metadata/Procedures.csv"), row.names = FALSE, quote = TRUE)
```

```{sql}
#| eval: false
-- Path to tables 
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/metadata/Procedures.csv'
INTO TABLE Procedures
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

### Genes Table

```{r}
# Subset the table, ensure unique rows, convert gene symbols to title case, and rename
genes <- data_table %>%
  mutate(gene_symbol = str_to_title(gene_symbol)) %>%  # Convert gene_symbol to title case
  select(gene_accession_id, gene_symbol) %>%          # Select specific columns
  distinct() %>%                                      # Keep only unique rows
  rename(gene_id = gene_accession_id)                 # Rename columns

# Save the table as CSV 
write.csv(genes, file = paste0(rootDir, "/data/metadata/Genes.csv"), row.names = FALSE, quote = TRUE)
```

```{sql}
LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/metadata/Genes.csv'
INTO TABLE Genes
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"'
IGNORE 1 LINES;
```

# R/Shiny

**Test R/Shiny 1:** They want the ability to select a particular knockout mouse and visualise the statistical scores of all phenotypes tested.

```{sql}
#| eval: false

SELECT Analyses.p_value, Parameters.parameter_name 
FROM Analyses 
JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id 
WHERE Analyses.gene_id IN (
SELECT gene_id 
FROM Genes 
WHERE gene_symbol = 'x')
ORDER BY Analyses.p_value ASC;
```

**Test R/Shiny 2:** The collaborator also wishes to visualise the statistical scores of all knockout mice for a selected phenotype.

```{sql}
#| eval: false

SELECT Analyses.p_value, Genes.gene_symbol 
FROM Analyses 
JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id 
JOIN Genes ON Analyses.gene_id = Genes.gene_id 
WHERE Parameters.parameter_name = 'x'
ORDER BY Analyses.p_value ASC;
```

**Test R/Shiny 3:** The collaborator would like to visualise clusters of genes with similar phenotype scores.  

```{sql}
#| eval: false

SELECT Genes.gene_symbol, Parameters.parameter_name, Analyses.p_value FROM Analyses JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id JOIN Genes ON Analyses.gene_id = Genes.gene_id; 
```
