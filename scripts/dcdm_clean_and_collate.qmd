---
title: "DCDM Project Log"
author: "Alicia"
format: html
editor: visual
---

# TRE Egress

Ella - is there anything to write here?

# Data Cleaning

```{r}
rm(list = ls())

# Install and load necessary packages
# install.packages(c("dplyr", "purrr", "readr")) # only do this once
library(dplyr)
library(purrr)
library(readr)
```

```{bash}
# Define working directory path in bash
rootDir="/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
```

```{r}
# Define the working directory path in r
rootDir = "/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
```

## Cleaning the Raw Data Files 

### Combining the Raw Data in a Table (Esta?) 

--\> This step can be done in bash after quality control (collating by Ella).

The dataset comprises thousands of CSV files. The key fields include: analysis_id, pvalue, parameter_name, gene_symbol, mouse_strain, parameter_id, gene_accession_id, mouse_life_stage.

To ensure efficient quality checks and facilitate further analysis, it is essential to combine all these files into a single, unified table based on their common fields.

Hereâ€™s a function that reads data from a file and transforms it into a row for a larger table:

```{r}
# Function to read a file and transform its contents into a single row
read_and_transform <- function(file) {
  
  # Read the file
  data <- read.csv(file, header = FALSE, col.names = c("field", "value"))
  # - header = FALSE: Tells R to treat the first row as regular data, not column names.
  # - col.names = c("field", "value"): Assigns consistent column names to facilitate clear and reliable subsetting in subsequent processing.

  # Convert all field names to lowercase for uniformity
  data$field <- tolower(data$field)
  
  # Transpose the 'value' column so that fields become column names
  transformed_data <- t(data$value) %>% 
    as.data.frame() %>% 
    setNames(data$field)
  
  # Rearrange columns to place 'analysis_id' as the first column
  transformed_data <- transformed_data %>%
    relocate(analysis_id, .before = everything())

  # Return the transformed data as a single-row data frame
  return(transformed_data)
}
```

The function can now be applied to all CSV files in the `raw_data` directory to combine them into a single table using the `map_dfr()` function from the **`purrr`** package.

```{r}
# List all CSV files in the raw_data directory
all_files <- list.files(paste0(rootDir, "data/raw_data"), pattern = "\\.csv$", full.names = TRUE)

# Apply the read_and_transform function to each file and combine results into a single data frame
data_table <- purrr::map_dfr(all_files, read_and_transform)
# Display the first 6 rows of the data table
View(data_table)

# Save the combined table to a new CSV file
write.csv(data_table, paste0(rootDir, "/temporary/data_table.csv"), row.names = FALSE)
```

### Quality Control/Cleaning Using the SOP (Esta)

The experiment data comes with an SOP specifying constraints for each field, including data type, minimum and maximum values for numerical data, length for strings, and specific values for certain fields. Data quality can be assessed by comparing the raw data to these predefined constraints.

First, have a look at the SOP to identify the constraints.

```{r}
SOP <- read.csv(paste0(rootDir, "data/metadata/IMPC_sop.csv"))
SOP
```

For each column, we need to check similar things, like whether the data is the correct type (e.g., string or number), whether it has the right length (for strings) or size (for floats), and whether it matches allowed values, like for `mouse_strain` and `mouse_life_stage`. We may also need to find missing values. For each issue, we want to print the value and its index. To make this process less repetitive, we can create reusable functions.

*Here, it would be good to check if the functions work as intended on values that should output an error (e.g. fake table containing rows each violating a single constraint for a parameter).*

```{r}
# These functions check if values meet specific criteria and return the indices of values that do not meet the criteria.

# Function to validate values based on the SOP type ("String" or "Float")
# For strings, it checks the string length; for numeric values, it checks the value directly.
validate_type_minmax <- function(x, dataType, min_val, max_val) {
  if (tolower(dataType) == "string") {
    which(sapply(x, nchar) < min_val | sapply(x, nchar) > max_val)
  } else if (tolower(dataType) == "float") {
    which(as.numeric(x) < min_val | as.numeric(x) > max_val)
  } 
}

# Check if values in a list are within the allowed values of a given vector
check_allowed_values <- function(values, allowed_values) {
  which(!values %in% allowed_values) # Find indices of invalid values
}

# Function to combine multiple lists of indices and return unique indices
combine_unique_indices <- function(...) {
  sort(unique(unlist(list(...))))
}

# Function to print the indices and values were issues were identified
print_invalid_indices_and_values <- function(x, variable_name, invalid_indices) {
  # Count the number of issues
  num_issues <- length(invalid_indices)
  
  # Print the contextual message
  cat("The '", variable_name, "' column has ", num_issues, " issue(s).\n", sep = "")
  
  # Print indices and values if there are any issues
  if (num_issues > 0) {
    cat("Indices:\n")
    print(invalid_indices)
    cat("Values:\n")
    print(x[invalid_indices])
  }
}
```

Now that we have defined these functions, we can loop through the data table fields, extract the corresponding constraints from the SOP, and validate them using the functions.

```{r}
# Extract field names from the data table
field_names <- colnames(data_table)
print(field_names)
  
# Loop through each field name
for (field_name in field_names) {
    cat("Processing field:", field_name, "\n")
    
    # Extract the corresponding row in the SOP
    sop_row <- SOP[SOP$dataField == field_name, ]
    
    # Access the column by name
    column <- data_table[[field_name]]
    
    # Validate the column based on SOP constraints
    SOP_constraints <- validate_type_minmax(column, sop_row$dataType, sop_row$minValue, sop_row$maxValue)
    
  # Check field-specific constraints
  # Initialize a variable for checking valid values
  field_specific_check <- integer(0) # Default to an empty vector

  if (field_name == "mouse_strain") {
    valid_mouse_strains <- c("C57BL", "B6J", "C3H", "129SV")
    field_specific_check <- check_allowed_values(column, valid_mouse_strains)
  } else if (field_name == "mouse_life_stage") {
    valid_mouse_life_stages <- c(
      "E12.5", "E15.5", "E18.5", "E9.5", 
      "Early adult", "Late adult", "Middle aged adult"
    )
    field_specific_check <- check_allowed_values(column, valid_mouse_life_stages)
  } else if (field_name == "analysis_id") {
    # Check if unique
    field_specific_check <- which(duplicated(column))
  } else if (field_name == "gene_symbol") {
    # Check if in title format
    field_specific_check <- which(!grepl("^[A-Z][a-zA-Z0-9]*$", column))
  }
    
    # Combine all indices of invalid values
    indices <- combine_unique_indices(SOP_constraints, field_specific_check)
    
    # Print the invalid indices and their corresponding values
    print_invalid_indices_and_values(column, field_name, indices)
}
```

*The next step is to handle the issues. Do we cap the p-values at 1? Do we remove the data from incorrect strains?*

## Cleaning the Metadata Files 

### `IMPC_procedure.txt (Alicia)`

The `IMPC_procedure.txt` file links each IMPC parameter to a procedure, such as Electrocardiogram (ECG), Histopathology, Insulin Blood Level, Fear Conditioning, and others. Each row in the table contains:

1.  A line number or unique procedure ID;
2.  A name;
3.  A description;
4.  A boolean indicating whether the procedure is mandatory;
5.  The corresponding IMPC parameter tested.

First, the line number column and the rest of the columns are surrounded by unnecessary double quotes (e.g., `"1""2,3,4,5"`). These can be removed, and a comma can be used to separate column 1 and column 2 (e.g., `1,2,3,4,5`).

Next, the description column contains commas, which are mistakenly interpreted as separators when reading the file, leading to errors (e.g., `1,2, 2, 2,3,4,5`). This issue can be resolved by identifying the description using a regular expression and enclosing it in double quotes (e.g., `1,"2, 2, 2",3,4,5`).

Finally, in the original file, the procedure ID variable is defined but does not exist in the data columns. Since the line numbers are also unique, the `line_number` and `procedureId` represent the same information. Therefore, the `line_number` variable can be removed and replaced with `procedureId`.

```{bash}
# Clean the IMPC_procedure.txt file.

# Define working directory path in bash
rootDir="/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"

# Step 1: Remove extra double quotes
# - Removes double quotes at the beginning and end of lines.
# - Replaces sequences of double quotes separated by spaces (`" "`) with commas.
sed 's/^"//;s/"$//;s/" "/,/g' "${rootDir}/data/metadata/IMPC_procedure.txt" | 

# Step 2: Enclose the 3rd column (description) in double quotes
# - The first group `([0-9]+,[^,]+,)` matches the line number and procedure name:
#   - `[0-9]+` matches the line number (a sequence of digits).
#   - `[^,]+` matches the procedure name (a sequence of characters not including commas).
# - The second group `(.*)` matches the description, which may contain commas or spaces.
# - The third group `(, (TRUE|FALSE), [0-9]{5})` matches:
#   - `(TRUE|FALSE)` for the boolean column (mandatory or not).
#   - `[0-9]{5}` for the IMPC parameter original ID (a 5-digit number).
sed -E 's/^([0-9]+,[^,]+,)(.*)(, (TRUE|FALSE), [0-9]{5})$/\1"\2"\3/' |

# Step 3: Remove the "line_number" column
sed -E 's/^line_number,//' > "${rootDir}/data/metadata/cleaned_IMPC_procedure.txt"
```

```{r}
# Load the cleaned procedure data, setting procedureId as row names
procedure <- read.table(
  file = paste0(rootDir, "data/metadata/cleaned_IMPC_procedure.txt"),
  header = TRUE,                
  sep = ",",                    
  quote = '"',                  
  row.names = "procedureId"     
)

# Display the first few rows of the data
head(procedure)
```

Since the file loads correctly in R, we can confirm that the regular expression (REGEX) accurately matched the columns when enclosing the description. This ensures that each column is in the correct format, so there is no need to further check the column formatting.

### `IMPC_parameter_description.txt (Sama)`

```{bash}
# Define working directory path in bash
rootDir="/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/"
sed -E 's/^"//; s/"$//; s/" "/,/g;' "${rootDir}/data/metadata/IMPC_parameter_description.txt" |
sed -E 's/^([0-9]{1,4},[0-9]{5}, )(.*)(,[^,]+, IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})$/\1"\2"\3/' |
sed -E 's/^x//' > "${rootDir}/data/metadata/cleaned_IMPC_parameter_description.txt"

```

```{r}
# Load the cleaned parameter data
parameter_description <- read.table(
  file = paste0(rootDir, "data/metadata/cleaned_IMPC_parameter_description.txt"), # File path
  header = TRUE,                 # First row contains column names
  sep = ",",                     # Fields are separated by commas
  stringsAsFactors = FALSE,      # Prevent character columns from converting to factors
  check.names = FALSE            # Keep column names as-is, even if they contain spaces or special characters
)

# Display the loaded data in a tabular format
View(parameter_description)
```

### `disease_information.txt (Sanj)`

```{bash}
# Define working directory path in bash
rootDir="/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3"
"${rootDir}/data/metadata/disease_information.txt" > "${rootDir}/data/metadata/cleaned_disease_information.txt"
```

```{r}

# Load the cleaned parameter data
disease_information <- read.table(
  file = paste0(rootDir, "data/metadata/disease_information.txt"), # File path
  header = TRUE,                 # First row contains column names
  sep = ",",                     # Fields are separated by commas
  quote = '"',                   # Handle quoted fields
  stringsAsFactors = FALSE,      # Prevent character columns from converting to factors
  check.names = FALSE            # Keep column names as-is, even if they contain spaces or special characters
)

# Display the loaded data in a tabular format
View(disease_information)

```

# Collating (Ella) 

# MySQL Database

# R/Shiny 
